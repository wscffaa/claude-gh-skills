#!/usr/bin/env python3
"""
å­¦æœ¯æ¶æ„å›¾ç”Ÿæˆå™¨ - æ–‡ä»¶ç³»ç»Ÿå³çŠ¶æ€æœºè®¾è®¡ v2.3

è®¾è®¡ç†å¿µï¼š
- æ¯ä¸ªä»»åŠ¡æœ‰ç‹¬ç«‹ç›®å½•ï¼Œç”¨æ–‡ä»¶å­˜åœ¨æ ‡è®°è¿›åº¦
- æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼šç¨‹åºéšæ—¶å¯ä¸­æ–­ï¼Œé‡å¯åè‡ªåŠ¨æ¥ç»­
- æ”¯æŒæ¸è¿›å¼è¿­ä»£ï¼šæ¯æ¬¡ä¿®æ”¹ç”Ÿæˆæ–°ç‰ˆæœ¬ï¼Œä¿ç•™å†å²
- æ”¯æŒè‡ªåŠ¨ä¸“å®¶å®¡é˜…ï¼šä¸‰ä¸“å®¶å¹¶è¡Œå®¡é˜…ï¼Œè‡ªåŠ¨è¿­ä»£ä¼˜åŒ–
- æè‡´é€æ˜ï¼šæ‰€æœ‰ä¸­é—´äº§ç‰©å¯æŸ¥çœ‹ï¼Œä¾¿äºè°ƒè¯•

ç›®å½•ç»“æ„:
experiments/visualizations/architecture/{task_id}/
â”œâ”€â”€ input.json              # [é˜¶æ®µ0] è¾“å…¥å‚æ•°å­˜æ¡£
â”œâ”€â”€ code_snapshot.py        # [é˜¶æ®µ0] ä»£ç å¿«ç…§
â”œâ”€â”€ analysis.md             # [é˜¶æ®µ1] ä»£ç åˆ†æç»“æœ
â”œâ”€â”€ architect_prompt.md     # [é˜¶æ®µ2] å‘ç»™ Architect çš„ prompt
â”œâ”€â”€ visual_schema.md        # [é˜¶æ®µ2] Visual Schema è¾“å‡º
â”œâ”€â”€ versions/               # [é˜¶æ®µ3] æ¸²æŸ“ç‰ˆæœ¬ç®¡ç†
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ renderer_prompt.md
â”‚   â”‚   â”œâ”€â”€ diagram.jpg
â”‚   â”‚   â”œâ”€â”€ response.txt
â”‚   â”‚   â””â”€â”€ review/                 # [é˜¶æ®µ3.5] ä¸“å®¶å®¡é˜…
â”‚   â”‚       â”œâ”€â”€ codex_prompt.md
â”‚   â”‚       â”œâ”€â”€ codex_review.md
â”‚   â”‚       â”œâ”€â”€ gemini_prompt.md
â”‚   â”‚       â”œâ”€â”€ gemini_review.md
â”‚   â”‚       â”œâ”€â”€ claude_prompt.md
â”‚   â”‚       â”œâ”€â”€ claude_review.md
â”‚   â”‚       â”œâ”€â”€ consensus.json      # å…±è¯†ç»“æœ
â”‚   â”‚       â””â”€â”€ iteration_decision.json  # è¿­ä»£å†³ç­–
â”‚   â”œâ”€â”€ v2/
â”‚   â”‚   â”œâ”€â”€ feedback.md     # ç”¨æˆ·è¿­ä»£åé¦ˆ
â”‚   â”‚   â”œâ”€â”€ renderer_prompt.md
â”‚   â”‚   â”œâ”€â”€ diagram.jpg
â”‚   â”‚   â””â”€â”€ response.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ latest_version.txt      # æœ€æ–°ç‰ˆæœ¬å·

ä½¿ç”¨æ–¹å¼:
    # æ–°å»ºä»»åŠ¡
    python3 skill.py --arch_code_path basicofr/archs/freqmamba_arch.py

    # æ¢å¤ä»»åŠ¡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    python3 skill.py --resume freqmamba_arch_20251231_160000

    # æ¸è¿›å¼è¿­ä»£ï¼ˆåŸºäºä¸Šä¸€ç‰ˆæœ¬ + åé¦ˆï¼‰
    python3 skill.py --resume task_id --iterate --feedback "ä¿®æ”¹æ–‡å­—æ ‡æ³¨ä¸ºæ•°å­¦ç¬¦å·é£æ ¼"

    # å¯ç”¨è‡ªåŠ¨ä¸“å®¶å®¡é˜…ï¼ˆä¸‰ä¸“å®¶å¹¶è¡Œå®¡é˜…ï¼Œè‡ªåŠ¨è¿­ä»£ç›´åˆ°é€šè¿‡ï¼‰
    python3 skill.py --arch_code_path arch.py --auto-review

    # è‡ªå®šä¹‰å®¡é˜…å‚æ•°
    python3 skill.py --resume task_id --auto-review --review-threshold 7 --max-iterations 3

    # åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡
    python3 skill.py --list

    # å¼ºåˆ¶é‡æ–°æ‰§è¡Œ
    python3 skill.py --resume freqmamba_arch_20251231_160000 --force
"""
import argparse
import json
import os
import re
import shutil
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List


# =============================================================================
# é…ç½®
# =============================================================================

DEFAULT_BASE_DIR = "experiments/visualizations/architecture"
LESSONS_SPEC_PATH = "specs/406-arch-diagram-lessons/README.md"


# =============================================================================
# Lessons Learned ç³»ç»Ÿ
# =============================================================================

def load_lessons_from_spec() -> List[Dict[str, Any]]:
    """ä» Spec æ–‡ä»¶è¯»å–å·²å®¡æ‰¹çš„ Lessons

    è¿”å›æ ¼å¼:
    [
        {
            "id": "LESSON-001",
            "title": "æ–‡æœ¬é‡å ",
            "category": "text-overlap",
            "pattern": "...",
            "solution": "...",
            "prompt_enhancement": "..."
        },
        ...
    ]
    """
    spec_path = Path(LESSONS_SPEC_PATH)
    if not spec_path.exists():
        # å°è¯•ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        spec_path = Path.cwd() / LESSONS_SPEC_PATH

    if not spec_path.exists():
        print(f"   âš ï¸  Lessons Spec ä¸å­˜åœ¨: {LESSONS_SPEC_PATH}", file=sys.stderr)
        return []

    try:
        with open(spec_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lessons = []
        import re

        # åŒ¹é… ### LESSON-XXX: æ ‡é¢˜ æ ¼å¼çš„å—
        lesson_pattern = r'### (LESSON-\d+): (.+?)\n(.*?)(?=\n### |\n---|\Z)'
        matches = re.findall(lesson_pattern, content, re.DOTALL)

        for lesson_id, title, body in matches:
            lesson = {
                "id": lesson_id,
                "title": title.strip(),
            }

            # æå–å„å­—æ®µ
            category_match = re.search(r'\*\*Category\*\*:\s*`([^`]+)`', body)
            if category_match:
                lesson["category"] = category_match.group(1)

            pattern_match = re.search(r'\*\*Pattern\*\*:\s*(.+?)(?=\n-|\Z)', body)
            if pattern_match:
                lesson["pattern"] = pattern_match.group(1).strip()

            solution_match = re.search(r'\*\*Solution\*\*:\s*(.+?)(?=\n-|\Z)', body)
            if solution_match:
                lesson["solution"] = solution_match.group(1).strip()

            # æå– Prompt Enhancementï¼ˆä»£ç å—å†…å®¹ï¼‰
            enhancement_match = re.search(r'\*\*Prompt Enhancement\*\*:\s*```\s*(.+?)```', body, re.DOTALL)
            if enhancement_match:
                lesson["prompt_enhancement"] = enhancement_match.group(1).strip()

            lessons.append(lesson)

        return lessons

    except Exception as e:
        print(f"   âš ï¸  è¯»å– Lessons Spec å¤±è´¥: {e}", file=sys.stderr)
        return []


def generate_lessons_prompt_section(lessons: List[Dict[str, Any]]) -> str:
    """å°† Lessons è½¬æ¢ä¸º Prompt æ³¨å…¥æ®µè½"""
    if not lessons:
        return ""

    lines = [
        "",
        "[LESSONS LEARNED - Historical Issues to Avoid]",
        "The following issues have been identified in previous generations. STRICTLY follow these guidelines:",
        ""
    ]

    for lesson in lessons:
        enhancement = lesson.get("prompt_enhancement", "")
        if enhancement:
            lines.append(f"# {lesson.get('id', 'LESSON')}: {lesson.get('title', 'Unknown')}")
            lines.append(enhancement)
            lines.append("")

    lines.append("[END LESSONS LEARNED]")
    lines.append("")

    return "\n".join(lines)


def extract_new_lessons_from_review(consensus: Dict[str, Any], reviews: Dict[str, str]) -> List[Dict[str, Any]]:
    """ä»ä¸“å®¶å®¡é˜…ä¸­æå–æ–°çš„æ½œåœ¨ Lessons

    è¿”å›å¾…å®¡æ‰¹çš„ Lesson åˆ—è¡¨
    """
    issues = consensus.get("issues", [])
    if not issues:
        return []

    # åªæå–é«˜ä¼˜å…ˆçº§é—®é¢˜ï¼ˆå¤šä¸“å®¶æåŠï¼‰
    high_priority = [i for i in issues if i.get("priority") == "high"]

    pending_lessons = []
    for i, issue in enumerate(high_priority[:3]):  # æœ€å¤š3æ¡
        pending_lessons.append({
            "id": f"PENDING-{i+1:03d}",
            "title": issue.get("issue", "Unknown")[:50],
            "category": "unknown",  # éœ€è¦ç”¨æˆ·åˆ†ç±»
            "pattern": issue.get("issue", ""),
            "mentioned_by": issue.get("mentioned_by", []),
            "proposed_solution": "",  # éœ€è¦ç”¨æˆ·å¡«å†™
            "proposed_enhancement": "",  # éœ€è¦ç”¨æˆ·å¡«å†™
        })

    return pending_lessons


def append_pending_lesson_to_spec(lesson: Dict[str, Any], task_id: str) -> bool:
    """å°†å¾…å®¡æ‰¹çš„ Lesson è¿½åŠ åˆ° Spec çš„ Pending åŒºåŸŸ"""
    spec_path = Path.cwd() / LESSONS_SPEC_PATH

    if not spec_path.exists():
        print(f"   âš ï¸  Lessons Spec ä¸å­˜åœ¨", file=sys.stderr)
        return False

    try:
        with open(spec_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æ‰¾åˆ° "Pending Lessons" åŒºåŸŸ
        pending_marker = "## Pending Lessons (å¾…å®¡æ‰¹)"
        if pending_marker not in content:
            print(f"   âš ï¸  æœªæ‰¾åˆ° Pending Lessons åŒºåŸŸ", file=sys.stderr)
            return False

        # æ„å»ºæ–°çš„ pending lesson æ¡ç›®
        new_entry = f"""
### {lesson['id']}: {lesson['title']}
- **Category**: `{lesson.get('category', 'unknown')}`
- **Pattern**: {lesson.get('pattern', 'TBD')}
- **Mentioned By**: {', '.join(lesson.get('mentioned_by', []))}
- **Proposed Solution**: *å¾…å¡«å†™*
- **Proposed Prompt Enhancement**:
  ```
  *å¾…å¡«å†™å…·ä½“çš„ Prompt æŒ‡ä»¤*
  ```
- **Discovered**: {datetime.now().strftime('%Y-%m-%d')}
- **Source**: {task_id}
"""

        # æ›¿æ¢ "*å½“å‰æ— å¾…å®¡æ‰¹é¡¹*" æˆ–è¿½åŠ åˆ° Pending åŒºåŸŸ
        if "*å½“å‰æ— å¾…å®¡æ‰¹é¡¹*" in content:
            content = content.replace("*å½“å‰æ— å¾…å®¡æ‰¹é¡¹*", new_entry.strip())
        else:
            # åœ¨ "---" åˆ†éš”ç¬¦å‰æ’å…¥
            insert_pos = content.find("---", content.find(pending_marker))
            if insert_pos > 0:
                content = content[:insert_pos] + new_entry + "\n" + content[insert_pos:]

        with open(spec_path, 'w', encoding='utf-8') as f:
            f.write(content)

        return True

    except Exception as e:
        print(f"   âš ï¸  è¿½åŠ  Pending Lesson å¤±è´¥: {e}", file=sys.stderr)
        return False


def get_next_lesson_number() -> int:
    """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„ Lesson ç¼–å·"""
    spec_path = Path.cwd() / LESSONS_SPEC_PATH
    if not spec_path.exists():
        return 1

    with open(spec_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾æ‰€æœ‰ LESSON-XXX ç¼–å·
    matches = re.findall(r'### LESSON-(\d+):', content)
    if not matches:
        return 1
    return max(int(m) for m in matches) + 1


def approve_lesson_to_spec(lesson: Dict[str, Any], task_id: str) -> bool:
    """å°† Lesson ç›´æ¥å†™å…¥å·²å®¡æ‰¹åŒºåŸŸï¼ˆåŒæ­¥å®¡æ‰¹æ¨¡å¼ï¼‰

    Args:
        lesson: åŒ…å« title, category, pattern, solution, prompt_enhancement çš„å­—å…¸
        task_id: æ¥æºä»»åŠ¡ ID

    Returns:
        æ˜¯å¦æˆåŠŸæ·»åŠ 
    """
    spec_path = Path.cwd() / LESSONS_SPEC_PATH

    if not spec_path.exists():
        print(f"   âš ï¸  Lessons Spec ä¸å­˜åœ¨", file=sys.stderr)
        return False

    try:
        with open(spec_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # è·å–ä¸‹ä¸€ä¸ªç¼–å·
        lesson_num = get_next_lesson_number()
        lesson_id = f"LESSON-{lesson_num:03d}"

        # æ„å»ºå·²å®¡æ‰¹ lesson æ¡ç›®
        prompt_enhancement = lesson.get('prompt_enhancement', '*å¾…å¡«å†™*')
        new_entry = f"""
### {lesson_id}: {lesson['title']}
- **Category**: `{lesson.get('category', 'unknown')}`
- **Pattern**: {lesson.get('pattern', 'TBD')}
- **Solution**: {lesson.get('solution', '*å¾…å¡«å†™*')}
- **Prompt Enhancement**:
  ```
  {prompt_enhancement}
  ```
- **Approved**: {datetime.now().strftime('%Y-%m-%d')}
- **Source**: {task_id}
"""

        # æ‰¾åˆ° "Lessons (ç”¨æˆ·å·²å®¡æ‰¹)" åŒºåŸŸçš„æœ«å°¾ï¼Œåœ¨ "Pending Lessons" ä¹‹å‰æ’å…¥
        pending_marker = "## Pending Lessons (å¾…å®¡æ‰¹)"
        if pending_marker in content:
            # åœ¨ Pending Lessons ä¹‹å‰æ’å…¥æ–° Lesson
            insert_pos = content.find(pending_marker)
            # å¾€å‰æ‰¾åˆ°æœ€åä¸€ä¸ª ---
            sep_pos = content.rfind("---", 0, insert_pos)
            if sep_pos > 0:
                content = content[:sep_pos] + new_entry + "\n" + content[sep_pos:]
            else:
                content = content[:insert_pos] + new_entry + "\n" + content[insert_pos:]

        with open(spec_path, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"   âœ… å·²æ·»åŠ  {lesson_id}: {lesson['title'][:30]}...")
        return True

    except Exception as e:
        print(f"   âš ï¸  æ·»åŠ  Lesson å¤±è´¥: {e}", file=sys.stderr)
        return False


# =============================================================================
# TaskState: æ–‡ä»¶ç³»ç»ŸçŠ¶æ€æœºï¼ˆæ”¯æŒç‰ˆæœ¬ç®¡ç†ï¼‰
# =============================================================================

class TaskState:
    """ä»»åŠ¡çŠ¶æ€ç®¡ç†å™¨ - ç”¨æ–‡ä»¶ç³»ç»Ÿè¿½è¸ªè¿›åº¦ï¼Œæ”¯æŒç‰ˆæœ¬ç®¡ç†"""

    def __init__(self, task_id: str, base_dir: str = DEFAULT_BASE_DIR):
        self.task_id = task_id
        self.task_dir = Path(base_dir) / task_id
        self.task_dir.mkdir(parents=True, exist_ok=True)
        self.versions_dir = self.task_dir / "versions"

    # --- æ–‡ä»¶è·¯å¾„å±æ€§ ---
    @property
    def input_json(self) -> Path:
        """è¾“å…¥å‚æ•°å­˜æ¡£"""
        return self.task_dir / "input.json"

    @property
    def code_snapshot(self) -> Path:
        """ä»£ç å¿«ç…§"""
        return self.task_dir / "code_snapshot.py"

    @property
    def analysis_md(self) -> Path:
        """ä»£ç åˆ†æç»“æœ"""
        return self.task_dir / "analysis.md"

    @property
    def architect_prompt(self) -> Path:
        """Architect å®Œæ•´ prompt"""
        return self.task_dir / "architect_prompt.md"

    @property
    def visual_schema(self) -> Path:
        """Visual Schema è¾“å‡º"""
        return self.task_dir / "visual_schema.md"

    @property
    def latest_version_file(self) -> Path:
        """æœ€æ–°ç‰ˆæœ¬å·æ–‡ä»¶"""
        return self.task_dir / "latest_version.txt"

    @property
    def review_config(self) -> Path:
        """å®¡é˜…é…ç½®æ–‡ä»¶"""
        return self.task_dir / "review_config.json"

    # --- ç‰ˆæœ¬ç®¡ç†æ–¹æ³• ---
    def get_latest_version(self) -> int:
        """è·å–æœ€æ–°ç‰ˆæœ¬å·"""
        if self.latest_version_file.exists():
            try:
                return int(self.latest_version_file.read_text().strip())
            except ValueError:
                pass

        # ä» versions ç›®å½•æ¨æ–­
        if self.versions_dir.exists():
            versions = [d.name for d in self.versions_dir.iterdir() if d.is_dir() and d.name.startswith('v')]
            if versions:
                nums = [int(v[1:]) for v in versions if v[1:].isdigit()]
                if nums:
                    return max(nums)
        return 0

    def set_latest_version(self, version: int):
        """è®¾ç½®æœ€æ–°ç‰ˆæœ¬å·"""
        self.latest_version_file.write_text(str(version))

    def get_version_dir(self, version: int) -> Path:
        """è·å–æŒ‡å®šç‰ˆæœ¬ç›®å½•"""
        return self.versions_dir / f"v{version}"

    def get_version_review_dir(self, version: int) -> Path:
        """è·å–æŒ‡å®šç‰ˆæœ¬çš„å®¡é˜…ç›®å½•"""
        return self.get_version_dir(version) / "review"

    def get_review_file(self, version: int, filename: str) -> Path:
        """è·å–å®¡é˜…ç›¸å…³æ–‡ä»¶è·¯å¾„"""
        return self.get_version_review_dir(version) / filename

    def get_latest_version_dir(self) -> Optional[Path]:
        """è·å–æœ€æ–°ç‰ˆæœ¬ç›®å½•"""
        version = self.get_latest_version()
        if version > 0:
            return self.get_version_dir(version)
        return None

    def create_new_version(self) -> tuple[int, Path]:
        """åˆ›å»ºæ–°ç‰ˆæœ¬ç›®å½•ï¼Œè¿”å› (ç‰ˆæœ¬å·, ç›®å½•è·¯å¾„)"""
        self.versions_dir.mkdir(parents=True, exist_ok=True)
        new_version = self.get_latest_version() + 1
        version_dir = self.get_version_dir(new_version)
        version_dir.mkdir(parents=True, exist_ok=True)
        return new_version, version_dir

    def get_version_diagram(self, version: int) -> Optional[Path]:
        """è·å–æŒ‡å®šç‰ˆæœ¬çš„æ¶æ„å›¾"""
        diagram = self.get_version_dir(version) / "diagram.jpg"
        return diagram if diagram.exists() else None

    def get_latest_diagram(self) -> Optional[Path]:
        """è·å–æœ€æ–°ç‰ˆæœ¬çš„æ¶æ„å›¾"""
        version = self.get_latest_version()
        if version > 0:
            return self.get_version_diagram(version)
        return None

    def list_versions(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬ä¿¡æ¯"""
        versions = []
        if not self.versions_dir.exists():
            return versions

        for vdir in sorted(self.versions_dir.iterdir()):
            if vdir.is_dir() and vdir.name.startswith('v'):
                try:
                    num = int(vdir.name[1:])
                    info = {
                        "version": num,
                        "dir": vdir,
                        "has_diagram": (vdir / "diagram.jpg").exists(),
                        "has_feedback": (vdir / "feedback.md").exists(),
                    }
                    # è·å–åˆ›å»ºæ—¶é—´
                    prompt_file = vdir / "renderer_prompt.md"
                    if prompt_file.exists():
                        info["created"] = datetime.fromtimestamp(prompt_file.stat().st_mtime)
                    versions.append(info)
                except ValueError:
                    continue

        return sorted(versions, key=lambda x: x["version"])

    # --- å…¼å®¹æ€§: æ—§ç‰ˆ diagram.jpg è¿ç§»åˆ° v1 ---
    def migrate_legacy_diagram(self):
        """å°†æ—§ç‰ˆ diagram.jpg è¿ç§»åˆ° versions/v1/"""
        legacy_diagram = self.task_dir / "diagram.jpg"
        legacy_prompt = self.task_dir / "renderer_prompt.md"
        legacy_response = self.task_dir / "response.txt"

        if legacy_diagram.exists() and not self.versions_dir.exists():
            print("ğŸ“¦ è¿ç§»æ—§ç‰ˆæ–‡ä»¶åˆ° versions/v1/...")
            v1_dir = self.get_version_dir(1)
            v1_dir.mkdir(parents=True, exist_ok=True)

            shutil.move(str(legacy_diagram), str(v1_dir / "diagram.jpg"))
            if legacy_prompt.exists():
                shutil.move(str(legacy_prompt), str(v1_dir / "renderer_prompt.md"))
            if legacy_response.exists():
                shutil.move(str(legacy_response), str(v1_dir / "response.txt"))

            self.set_latest_version(1)
            print(f"   âœ“ å·²è¿ç§»åˆ°: {v1_dir}")

    # --- çŠ¶æ€æ£€æŸ¥ ---
    def stage_complete(self, stage: str) -> bool:
        """æ£€æŸ¥æŸé˜¶æ®µæ˜¯å¦å®Œæˆï¼ˆæ–‡ä»¶å­˜åœ¨å³å®Œæˆï¼‰"""
        stage_files = {
            "input": self.input_json,
            "snapshot": self.code_snapshot,
            "analysis": self.analysis_md,
            "schema": self.visual_schema,
            "diagram": self.get_latest_diagram(),
        }
        target = stage_files.get(stage)
        return target is not None and target.exists()

    def review_stage_complete(self, version: int, stage: str) -> bool:
        """æ£€æŸ¥ç‰ˆæœ¬çš„å®¡é˜…é˜¶æ®µæ˜¯å¦å®Œæˆ

        stage å¯é€‰å€¼:
        - "codex": codex_review.md å­˜åœ¨
        - "gemini": gemini_review.md å­˜åœ¨
        - "claude": claude_review.md å­˜åœ¨
        - "consensus": consensus.json å­˜åœ¨
        - "decision": iteration_decision.json å­˜åœ¨
        """
        review_dir = self.get_version_review_dir(version)
        stage_files = {
            "codex": "codex_review.md",
            "gemini": "gemini_review.md",
            "claude": "claude_review.md",
            "consensus": "consensus.json",
            "decision": "iteration_decision.json",
        }
        filename = stage_files.get(stage)
        if not filename:
            return False
        return (review_dir / filename).exists()

    def get_review_status(self, version: int) -> Dict[str, bool]:
        """è·å–ç‰ˆæœ¬çš„æ‰€æœ‰å®¡é˜…çŠ¶æ€"""
        return {
            "codex": self.review_stage_complete(version, "codex"),
            "gemini": self.review_stage_complete(version, "gemini"),
            "claude": self.review_stage_complete(version, "claude"),
            "consensus": self.review_stage_complete(version, "consensus"),
            "decision": self.review_stage_complete(version, "decision"),
        }

    def get_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰é˜¶æ®µçŠ¶æ€"""
        latest_version = self.get_latest_version()
        latest_diagram = self.get_latest_diagram()

        return {
            "input": self.input_json.exists(),
            "snapshot": self.code_snapshot.exists(),
            "analysis": self.analysis_md.exists(),
            "architect_prompt": self.architect_prompt.exists(),
            "schema": self.visual_schema.exists(),
            "latest_version": latest_version,
            "has_diagram": latest_diagram is not None and latest_diagram.exists(),
        }

    def print_status(self):
        """æ‰“å°å½“å‰çŠ¶æ€"""
        status = self.get_status()
        print(f"\nğŸ¯ Task: {self.task_id}")
        print("â”" * 50)

        stages = [
            ("input", "input.json", "ä¿å­˜è¾“å…¥å‚æ•°"),
            ("snapshot", "code_snapshot.py", "ä»£ç å¿«ç…§"),
            ("analysis", "analysis.md", "ä»£ç åˆ†æ"),
            ("architect_prompt", "architect_prompt.md", "Architect Prompt"),
            ("schema", "visual_schema.md", "Visual Schema"),
        ]

        for key, filename, desc in stages:
            icon = "âœ“" if status[key] else " "
            print(f"[{icon}] {filename:<22} ({desc})")

        # ç‰ˆæœ¬ä¿¡æ¯
        print("â”" * 50)
        versions = self.list_versions()
        if versions:
            print(f"ğŸ“‚ ç‰ˆæœ¬å†å² ({len(versions)} ä¸ªç‰ˆæœ¬):")
            for v in versions:
                icon = "âœ“" if v["has_diagram"] else "â³"
                feedback_mark = " [æœ‰åé¦ˆ]" if v["has_feedback"] else ""
                created = v.get("created", "")
                if created:
                    created = created.strftime("%H:%M:%S")
                print(f"   [{icon}] v{v['version']:<3} {created}{feedback_mark}")
                version_dir = v["dir"]
                diagram_file = version_dir / "diagram.jpg"
                version_sub_items = []

                if diagram_file.exists():
                    size_mb = diagram_file.stat().st_size / (1024 * 1024)
                    version_sub_items.append({
                        "type": "diagram",
                        "text": f"diagram.jpg ({size_mb:.1f}MB)"
                    })

                review_status = self.get_review_status(v["version"])
                consensus_data = self.load_consensus(v["version"])
                decision_data = self.load_iteration_decision(v["version"])
                has_review = any(review_status.values()) or consensus_data is not None or decision_data is not None

                if has_review:
                    def _format_score(val: Any) -> Optional[str]:
                        if isinstance(val, (int, float)):
                            return f"{val:.1f}" if isinstance(val, float) else str(val)
                        return None

                    review_icon = "âœ…" if review_status.get("consensus") or review_status.get("decision") else "â³"
                    avg_score = None
                    stage_scores: Dict[str, Any] = {}

                    if isinstance(consensus_data, dict):
                        avg_score = consensus_data.get("average_score") or consensus_data.get("score")
                        scores_map = consensus_data.get("scores")
                        if isinstance(scores_map, dict):
                            stage_scores.update(scores_map)
                        else:
                            for key in ("codex", "gemini", "claude", "consensus"):
                                if key in consensus_data:
                                    stage_scores[key] = consensus_data.get(key)
                        if avg_score is None and stage_scores:
                            numeric_scores = [s for s in stage_scores.values() if isinstance(s, (int, float))]
                            if numeric_scores:
                                avg_score = sum(numeric_scores) / len(numeric_scores)

                    score_text = _format_score(avg_score)
                    decision_text = None
                    if isinstance(decision_data, dict):
                        decision_text = decision_data.get("decision") or decision_data.get("action") or decision_data.get("result")
                    if decision_text is None and isinstance(consensus_data, dict):
                        decision_text = consensus_data.get("decision") or consensus_data.get("action")

                    review_summary_parts = []
                    if score_text:
                        review_summary_parts.append(f"{score_text}/10")
                    if decision_text:
                        arrow = "â†’ " if review_summary_parts else ""
                        review_summary_parts.append(f"{arrow}{decision_text}")
                    review_summary = " ".join(review_summary_parts).strip()
                    summary_line = f"review: {review_icon}"
                    if review_summary:
                        summary_line += f" {review_summary}"

                    stage_lines = []
                    for stage_key, stage_label in [
                        ("codex", "codex"),
                        ("gemini", "gemini"),
                        ("claude", "claude"),
                        ("consensus", "consensus"),
                        ("decision", "decision"),
                    ]:
                        stage_icon = "âœ“" if review_status.get(stage_key) else " "
                        line = f"[{stage_icon}] {stage_label}"
                        stage_score = _format_score(stage_scores.get(stage_key))
                        if stage_key in ("codex", "gemini", "claude") and stage_score:
                            line += f": {stage_score}/10"
                        elif stage_key == "consensus" and score_text:
                            line += f": {score_text}/10"
                        elif stage_key == "decision" and decision_text:
                            line += f": {decision_text}"
                        stage_lines.append(line)

                    version_sub_items.append({
                        "type": "review",
                        "text": summary_line.strip(),
                        "lines": stage_lines
                    })

                for idx, item in enumerate(version_sub_items):
                    connector = "â””â”€â”€" if idx == len(version_sub_items) - 1 else "â”œâ”€â”€"
                    prefix = f"        {connector} "
                    if item["type"] == "diagram":
                        print(f"{prefix}{item['text']}")
                    elif item["type"] == "review":
                        print(f"{prefix}{item['text']}")
                        nested_prefix = "        " + ("    " if idx == len(version_sub_items) - 1 else "â”‚   ")
                        for ridx, line in enumerate(item["lines"]):
                            nested_connector = "â””â”€â”€" if ridx == len(item["lines"]) - 1 else "â”œâ”€â”€"
                            print(f"{nested_prefix}{nested_connector} {line}")
        else:
            print("ğŸ“‚ ç‰ˆæœ¬å†å²: (æ— )")

        print("â”" * 50)

    def clear(self):
        """æ¸…é™¤æ‰€æœ‰æ–‡ä»¶ï¼ˆå¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼‰"""
        if self.task_dir.exists():
            shutil.rmtree(self.task_dir)
        self.task_dir.mkdir(parents=True, exist_ok=True)

    def clear_versions(self):
        """åªæ¸…é™¤ç‰ˆæœ¬ç›®å½•ï¼ˆä¿ç•™åˆ†æå’Œ schemaï¼‰"""
        if self.versions_dir.exists():
            shutil.rmtree(self.versions_dir)
        self.latest_version_file.unlink(missing_ok=True)

    def load_input(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½å·²ä¿å­˜çš„è¾“å…¥å‚æ•°"""
        if self.input_json.exists():
            with open(self.input_json, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None

    def load_consensus(self, version: int) -> Optional[Dict[str, Any]]:
        """åŠ è½½ç‰ˆæœ¬çš„å…±è¯†ç»“æœ"""
        consensus_file = self.get_review_file(version, "consensus.json")
        if consensus_file.exists():
            with open(consensus_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return None

    def load_iteration_decision(self, version: int) -> Optional[Dict[str, Any]]:
        """åŠ è½½ç‰ˆæœ¬çš„è¿­ä»£å†³ç­–"""
        decision_file = self.get_review_file(version, "iteration_decision.json")
        if decision_file.exists():
            with open(decision_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return None


# =============================================================================
# æ¨¡æ¿å’Œé…ç½®åŠ è½½ï¼ˆä¿æŒåŸæœ‰å‡½æ•°ï¼‰
# =============================================================================

def load_architect_template(inject_lessons: bool = True) -> str:
    """åŠ è½½ Architect Prompt æ¨¡æ¿ï¼Œå¯é€‰æ³¨å…¥å†å² Lessons"""
    skill_template = Path(__file__).parent.parent / "templates" / "architect_prompt.txt"

    template_content = ""
    if skill_template.exists():
        with open(skill_template, 'r', encoding='utf-8') as f:
            template_content = f.read()
    else:
        project_template = Path.cwd() / "01_Architect_Prompt_Full.md"
        if project_template.exists():
            print(f"âš ï¸  ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•æ¨¡æ¿: {project_template}", file=sys.stderr)
            with open(project_template, 'r', encoding='utf-8') as f:
                content = f.read()
                if "```" in content:
                    start = content.find("```") + 3
                    end = content.rfind("```")
                    if end > start:
                        template_content = content[start:end].strip()
                else:
                    template_content = content
        else:
            print(f"âŒ æœªæ‰¾åˆ° Architect æ¨¡æ¿æ–‡ä»¶", file=sys.stderr)
            sys.exit(1)

    # æ³¨å…¥å†å² Lessons
    if inject_lessons:
        lessons = load_lessons_from_spec()
        if lessons:
            lessons_section = generate_lessons_prompt_section(lessons)
            print(f"   ğŸ“š å·²æ³¨å…¥ {len(lessons)} æ¡å†å²ç»éªŒåˆ° Architect Prompt")
            # åœ¨æ¨¡æ¿æœ«å°¾æ·»åŠ  Lessons
            template_content = template_content + "\n" + lessons_section

    return template_content


def load_renderer_template(use_advanced: bool = False, inject_lessons: bool = True) -> str:
    """åŠ è½½ Renderer Prompt æ¨¡æ¿ï¼Œå¯é€‰æ³¨å…¥å†å² Lessons"""
    template_name = "renderer_prompt_advanced.txt" if use_advanced else "renderer_prompt_basic.txt"
    skill_template = Path(__file__).parent.parent / "templates" / template_name

    template_content = ""
    if skill_template.exists():
        with open(skill_template, 'r', encoding='utf-8') as f:
            template_content = f.read()
    else:
        project_template = Path.cwd() / "02_Renderer_Prompt_Full.md"
        if project_template.exists():
            print(f"âš ï¸  ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•æ¨¡æ¿: {project_template}", file=sys.stderr)
            with open(project_template, 'r', encoding='utf-8') as f:
                content = f.read()
                if "```" in content:
                    start = content.find("```") + 3
                    end = content.find("```", start)
                    if end > start:
                        template_content = content[start:end].strip()
                else:
                    template_content = content
        else:
            print(f"âŒ æœªæ‰¾åˆ° Renderer æ¨¡æ¿æ–‡ä»¶", file=sys.stderr)
            sys.exit(1)

    # æ³¨å…¥å†å² Lessons
    if inject_lessons:
        lessons = load_lessons_from_spec()
        if lessons:
            lessons_section = generate_lessons_prompt_section(lessons)
            print(f"   ğŸ“š å·²æ³¨å…¥ {len(lessons)} æ¡å†å²ç»éªŒåˆ° Renderer Prompt")
            # åœ¨æ¨¡æ¿å¼€å¤´æ·»åŠ  Lessonsï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
            template_content = lessons_section + "\n" + template_content

    return template_content


def load_iteration_template() -> str:
    """åŠ è½½è¿­ä»£ Prompt æ¨¡æ¿"""
    template_path = Path(__file__).parent.parent / "templates" / "renderer_prompt_iterate.txt"

    if template_path.exists():
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()

    # é»˜è®¤è¿­ä»£æ¨¡æ¿
    return """**Architecture Diagram Iterative Refinement**

I have provided the previous version of the architecture diagram. Please refine it based on the following feedback while maintaining all correct elements.

**IMPORTANT: This is an ITERATIVE refinement task.**
- Keep everything that is correct from the previous version
- Only modify the specific aspects mentioned in the feedback
- Maintain the same overall layout and structure unless explicitly asked to change

**Previous Version Feedback & Refinement Request:**
{feedback}

**Original Visual Schema (for reference):**
{visual_schema_content}

**Style Requirements:**
- Maintain CVPR/NeurIPS academic standard
- Use mathematical notation for labels (e.g., $X_t$, $M_{fused}$)
- Keep 3D isometric cuboids for feature maps
- Light pastel colors, thin black outlines
- Clean legend at bottom

**Output:** Generate the refined architecture diagram incorporating the feedback while preserving all correct elements from the previous version.
"""


def load_model_config() -> Dict[str, str]:
    """åŠ è½½æ¨¡å‹é…ç½®"""
    config_file = Path(__file__).parent / 'config.json'
    default_config = {
        'architect': 'gpt-5.2',
        'renderer': 'gemini-3-pro-image-16x9-4k'
    }

    if not config_file.exists():
        return default_config

    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)

        active_profile = config_data.get('active_profile', 'default')
        profile = config_data.get('profiles', {}).get(active_profile, {})

        return {
            'architect': profile.get('architect', default_config['architect']),
            'renderer': profile.get('renderer', default_config['renderer'])
        }
    except Exception as e:
        print(f"âš ï¸  é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}", file=sys.stderr)
        return default_config


# =============================================================================
# Pipeline é˜¶æ®µå‡½æ•°
# =============================================================================

def save_input(state: TaskState, args: argparse.Namespace):
    """é˜¶æ®µ0: ä¿å­˜è¾“å…¥å‚æ•°"""
    print("\nğŸ“¥ [é˜¶æ®µ0] ä¿å­˜è¾“å…¥å‚æ•°...")

    input_data = {
        "task_id": state.task_id,
        "created_at": datetime.now().isoformat(),
        "arch_code_path": args.arch_code_path,
        "paper_content": args.paper_content,
        "model_architect": args.model_architect,
        "model_renderer": args.model_renderer,
        "reference_images": args.reference_images,
    }

    with open(state.input_json, 'w', encoding='utf-8') as f:
        json.dump(input_data, f, ensure_ascii=False, indent=2)

    print(f"   âœ“ å·²ä¿å­˜: {state.input_json}")

    # ä¿å­˜ä»£ç å¿«ç…§
    if args.arch_code_path and Path(args.arch_code_path).exists():
        shutil.copy(args.arch_code_path, state.code_snapshot)
        print(f"   âœ“ ä»£ç å¿«ç…§: {state.code_snapshot}")


def run_analysis(state: TaskState):
    """é˜¶æ®µ1: ä»£ç åˆ†æ"""
    print("\nğŸ” [é˜¶æ®µ1] ä»£ç åˆ†æ...")

    input_data = state.load_input()
    if not input_data:
        print("âŒ æœªæ‰¾åˆ°è¾“å…¥å‚æ•°", file=sys.stderr)
        return False

    arch_code_path = input_data.get('arch_code_path')
    if not arch_code_path:
        # å¦‚æœæ²¡æœ‰ä»£ç è·¯å¾„ï¼Œä½¿ç”¨ paper_content
        paper_content = input_data.get('paper_content', '')
        if paper_content:
            with open(state.analysis_md, 'w', encoding='utf-8') as f:
                f.write(f"# è®ºæ–‡å†…å®¹åˆ†æ\n\n{paper_content}")
            print(f"   âœ“ ç›´æ¥ä½¿ç”¨è®ºæ–‡å†…å®¹: {state.analysis_md}")
            return True
        print("âŒ æ— è¾“å…¥å†…å®¹", file=sys.stderr)
        return False

    analysis_prompt = f"""åˆ†ææ¶æ„ä»£ç  @{arch_code_path}ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ç”¨äºç”Ÿæˆæ¶æ„å›¾ï¼š

1. **æ ¸å¿ƒæ¨¡å—è¯†åˆ«**:
   - åˆ—å‡ºä¸»è¦çš„ç½‘ç»œæ¨¡å—ç±»ï¼ˆå¦‚ Encoder, Decoder, Attention, etc.ï¼‰
   - è¯†åˆ«åˆ›æ–°æ¨¡å—ï¼ˆæ ‡æ³¨ä¸º Innovation A/B/Cï¼‰

2. **æ•°æ®æµåˆ†æ**:
   - è¾“å…¥å¼ é‡å½¢çŠ¶å’Œç±»å‹
   - ä¸»è¦çš„å‰å‘ä¼ æ’­è·¯å¾„
   - è¾“å‡ºå¼ é‡å½¢çŠ¶å’Œç±»å‹
   - å…³é”®çš„ä¸­é—´ç‰¹å¾ï¼ˆfeature mapsï¼‰

3. **åˆ›æ–°ç‚¹æ ‡æ³¨**:
   - è¯†åˆ«è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®æ¨¡å—
   - æ ‡è®°è¿™äº›æ¨¡å—åœ¨æ•°æ®æµä¸­çš„ä½ç½®

4. **æ¶æ„æè¿°**:
   - ç”¨è‡ªç„¶è¯­è¨€æè¿°æ•´ä½“æ¶æ„å¸ƒå±€
   - é€‚åˆç”¨äº Architect Prompt çš„è¾“å…¥æ ¼å¼

è¾“å‡ºæ ¼å¼ï¼šæ¸…æ™°çš„åˆ†æ®µ Markdown æ–‡æœ¬ï¼ŒåŒ…å«ä¸Šè¿° 4 ä¸ªéƒ¨åˆ†ã€‚
"""

    try:
        print(f"   è°ƒç”¨ Codex åˆ†æ: {arch_code_path}")

        result = subprocess.run(
            ["codex", "exec", "-"],
            input=analysis_prompt,
            capture_output=True,
            text=True,
            timeout=300
        )

        if result.returncode != 0:
            print(f"âŒ ä»£ç åˆ†æå¤±è´¥: {result.stderr}", file=sys.stderr)
            return False

        analysis_result = result.stdout.strip()

        with open(state.analysis_md, 'w', encoding='utf-8') as f:
            f.write(f"# ä»£ç åˆ†æç»“æœ\n\n")
            f.write(f"**æºæ–‡ä»¶**: `{arch_code_path}`\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")
            f.write(analysis_result)

        print(f"   âœ“ å·²ä¿å­˜: {state.analysis_md} ({len(analysis_result)} chars)")
        return True

    except subprocess.TimeoutExpired:
        print("âŒ ä»£ç åˆ†æè¶…æ—¶", file=sys.stderr)
        return False
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ° codex CLI", file=sys.stderr)
        return False
    except Exception as e:
        print(f"âŒ ä»£ç åˆ†æå¼‚å¸¸: {e}", file=sys.stderr)
        return False


def run_architect(state: TaskState):
    """é˜¶æ®µ2: ç”Ÿæˆ Visual Schema"""
    print("\nğŸ—ï¸  [é˜¶æ®µ2] ç”Ÿæˆ Visual Schema...")

    # è¯»å–åˆ†æç»“æœ
    if not state.analysis_md.exists():
        print("âŒ æœªæ‰¾åˆ°ä»£ç åˆ†æç»“æœ", file=sys.stderr)
        return False

    with open(state.analysis_md, 'r', encoding='utf-8') as f:
        paper_content = f.read()

    input_data = state.load_input()
    model = input_data.get('model_architect', 'gpt-5.2') if input_data else 'gpt-5.2'

    # åŠ è½½æ¨¡æ¿å¹¶æ„å»º prompt
    architect_template = load_architect_template()
    full_prompt = architect_template.replace("{paper_content}", paper_content)

    # ä¿å­˜å®Œæ•´ promptï¼ˆä¾¿äºè°ƒè¯•ï¼‰
    with open(state.architect_prompt, 'w', encoding='utf-8') as f:
        f.write(f"# Architect Prompt\n\n")
        f.write(f"**æ¨¡å‹**: {model}\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}\n\n")
        f.write("---\n\n")
        f.write(full_prompt)
    print(f"   âœ“ Prompt å·²ä¿å­˜: {state.architect_prompt}")

    try:
        print(f"   è°ƒç”¨ {model} ç”Ÿæˆ Visual Schema...")

        env = os.environ.copy()
        env["CODEX_MODEL"] = model

        result = subprocess.run(
            ["codex", "exec", "-"],
            input=full_prompt,
            capture_output=True,
            text=True,
            env=env,
            timeout=300
        )

        if result.returncode != 0:
            print(f"âŒ Architect è°ƒç”¨å¤±è´¥: {result.stderr}", file=sys.stderr)
            return False

        visual_schema = result.stdout.strip()

        # ä¿å­˜ Visual Schema
        with open(state.visual_schema, 'w', encoding='utf-8') as f:
            f.write(f"# Visual Schema\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}\n")
            f.write(f"**æ¨¡å‹**: {model}\n\n")
            f.write("---\n\n")
            f.write(visual_schema)

        print(f"   âœ“ å·²ä¿å­˜: {state.visual_schema} ({len(visual_schema)} chars)")
        return True

    except subprocess.TimeoutExpired:
        print("âŒ Architect è°ƒç”¨è¶…æ—¶", file=sys.stderr)
        return False
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ° codex CLI", file=sys.stderr)
        return False
    except Exception as e:
        print(f"âŒ Architect è°ƒç”¨å¼‚å¸¸: {e}", file=sys.stderr)
        return False


def run_renderer(state: TaskState, feedback: Optional[str] = None, extra_reference_images: Optional[List[str]] = None):
    """é˜¶æ®µ3: æ¸²æŸ“æ¶æ„å›¾ï¼ˆæ”¯æŒç‰ˆæœ¬è¿­ä»£ï¼‰

    è¿­ä»£æ¨¡å¼ä½¿ç”¨ Gemini çš„ inpainting åŠŸèƒ½ï¼š
    - å°†ä¸Šä¸€ç‰ˆæœ¬å›¾ä½œä¸ºåŸºç¡€å›¾ï¼ˆç¬¬ä¸€ä¸ª content partï¼‰
    - ä½¿ç”¨ç¼–è¾‘æŒ‡ä»¤è€Œéé‡æ–°ç”Ÿæˆ
    - æ¨¡å‹ä¼šåŸºäºåŸå›¾è¿›è¡Œå±€éƒ¨ä¿®æ”¹ï¼Œä¿æŒè¿ç»­æ€§
    """

    # è¿ç§»æ—§ç‰ˆæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    state.migrate_legacy_diagram()

    # åˆ›å»ºæ–°ç‰ˆæœ¬
    new_version, version_dir = state.create_new_version()
    is_iteration = new_version > 1 and feedback is not None

    if is_iteration:
        print(f"\nğŸ¨ [é˜¶æ®µ3] æ¸²æŸ“æ¶æ„å›¾ v{new_version}ï¼ˆå›¾åƒç¼–è¾‘æ¨¡å¼ï¼‰...")
    else:
        print(f"\nğŸ¨ [é˜¶æ®µ3] æ¸²æŸ“æ¶æ„å›¾ v{new_version}...")

    # è¯»å– Visual Schema
    if not state.visual_schema.exists():
        print("âŒ æœªæ‰¾åˆ° Visual Schema", file=sys.stderr)
        return False

    with open(state.visual_schema, 'r', encoding='utf-8') as f:
        visual_schema = f.read()

    input_data = state.load_input()
    model = input_data.get('model_renderer', 'gemini-3-pro-image-16x9-4k') if input_data else 'gemini-3-pro-image-16x9-4k'
    reference_images = input_data.get('reference_images') if input_data else None

    # åˆå¹¶é¢å¤–å‚è€ƒå›¾
    if extra_reference_images:
        reference_images = (reference_images or []) + extra_reference_images

    # æå– Schema æ ¸å¿ƒå†…å®¹
    schema_content = visual_schema
    if "---BEGIN PROMPT---" in visual_schema:
        start_idx = visual_schema.find("---BEGIN PROMPT---") + len("---BEGIN PROMPT---")
        end_idx = visual_schema.find("---END PROMPT---")
        if end_idx > start_idx:
            schema_content = visual_schema[start_idx:end_idx].strip()

    # è·å–ä¸Šä¸€ç‰ˆæœ¬å›¾ï¼ˆç”¨äºè¿­ä»£ç¼–è¾‘ï¼‰
    prev_diagram = None
    if is_iteration:
        prev_diagram = state.get_version_diagram(new_version - 1)
        if prev_diagram and prev_diagram.exists():
            print(f"   ğŸ“ åŸºç¡€å›¾ï¼ˆå°†è¢«ç¼–è¾‘ï¼‰: v{new_version - 1}")
        else:
            print(f"   âš ï¸  æœªæ‰¾åˆ°ä¸Šä¸€ç‰ˆæœ¬å›¾ï¼Œå°†é‡æ–°ç”Ÿæˆ")
            is_iteration = False

    # æ„å»º prompt
    if is_iteration:
        # è¿­ä»£æ¨¡å¼ï¼šä½¿ç”¨ inpainting é£æ ¼çš„ç¼–è¾‘æŒ‡ä»¤
        iteration_template = load_iteration_template()
        full_prompt = iteration_template.replace("{feedback}", feedback).replace("{visual_schema_content}", schema_content)

        # ä¿å­˜åé¦ˆ
        feedback_file = version_dir / "feedback.md"
        with open(feedback_file, 'w', encoding='utf-8') as f:
            f.write(f"# è¿­ä»£åé¦ˆ v{new_version}\n\n")
            f.write(f"**åŸºäºç‰ˆæœ¬**: v{new_version - 1}\n")
            f.write(f"**ç¼–è¾‘æ¨¡å¼**: inpainting\n")
            f.write(f"**æ—¶é—´**: {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")
            f.write(feedback)
        print(f"   âœ“ åé¦ˆå·²ä¿å­˜: {feedback_file}")
    else:
        # é¦–æ¬¡ç”Ÿæˆæˆ–éè¿­ä»£ï¼šä½¿ç”¨æ ‡å‡†æ¨¡æ¿
        use_advanced = reference_images is not None and len(reference_images) > 0
        renderer_template = load_renderer_template(use_advanced=use_advanced)
        full_prompt = renderer_template.replace("{visual_schema_content}", schema_content)

    # ä¿å­˜å®Œæ•´ prompt
    renderer_prompt_file = version_dir / "renderer_prompt.md"
    with open(renderer_prompt_file, 'w', encoding='utf-8') as f:
        f.write(f"# Renderer Prompt v{new_version}\n\n")
        f.write(f"**æ¨¡å‹**: {model}\n")
        f.write(f"**å‚è€ƒå›¾**: {reference_images}\n")
        f.write(f"**è¿­ä»£æ¨¡å¼**: {is_iteration}\n")
        f.write(f"**ç¼–è¾‘åŸºç¡€å›¾**: {prev_diagram if is_iteration else 'N/A'}\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}\n\n")
        f.write("---\n\n")
        f.write(full_prompt)
    print(f"   âœ“ Prompt å·²ä¿å­˜: {renderer_prompt_file}")

    try:
        from openai import OpenAI
        import base64

        print(f"   è¿æ¥ Gemini API...")

        client = OpenAI(
            base_url="http://127.0.0.1:8888/v1",
            api_key=os.getenv("OPENAI_API_KEY", "sk-placeholder")
        )

        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content_parts = []

        # è¿­ä»£æ¨¡å¼ï¼šå…ˆæ”¾åŸºç¡€å›¾ï¼ˆinpainting çš„å…³é”®ï¼‰
        if is_iteration and prev_diagram:
            try:
                with open(prev_diagram, 'rb') as img_file:
                    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')
                    img_type = "image/png" if str(prev_diagram).lower().endswith('.png') else "image/jpeg"
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:{img_type};base64,{img_base64}"}
                    })
                    print(f"   ğŸ“ å·²åŠ è½½åŸºç¡€å›¾ï¼ˆç”¨äºç¼–è¾‘ï¼‰: {prev_diagram}")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½åŸºç¡€å›¾å¤±è´¥: {e}", file=sys.stderr)

        # æ·»åŠ æ–‡æœ¬ prompt
        content_parts.append({"type": "text", "text": full_prompt})

        # æ·»åŠ å‚è€ƒå›¾ï¼ˆé£æ ¼å‚è€ƒï¼‰
        if reference_images:
            for img_path in reference_images:
                try:
                    with open(img_path, 'rb') as img_file:
                        img_base64 = base64.b64encode(img_file.read()).decode('utf-8')
                        img_type = "image/png" if img_path.lower().endswith('.png') else "image/jpeg"
                        content_parts.append({
                            "type": "image_url",
                            "image_url": {"url": f"data:{img_type};base64,{img_base64}"}
                        })
                        print(f"   ğŸ“ å·²åŠ è½½å‚è€ƒå›¾ï¼ˆé£æ ¼å¼•å¯¼ï¼‰: {img_path}")
                except Exception as e:
                    print(f"âš ï¸  åŠ è½½å‚è€ƒå›¾å¤±è´¥ {img_path}: {e}", file=sys.stderr)

        print(f"   å‘é€æ¸²æŸ“è¯·æ±‚ï¼ˆ{'ç¼–è¾‘æ¨¡å¼' if is_iteration else 'ç”Ÿæˆæ¨¡å¼'}ï¼‰...")

        response = client.chat.completions.create(
            model=model,
            extra_body={"size": "1216x896"},
            messages=[{"role": "user", "content": content_parts}]
        )

        result = response.choices[0].message.content

        # ä¿å­˜å®Œæ•´å“åº”
        response_file = version_dir / "response.txt"
        with open(response_file, 'w', encoding='utf-8') as f:
            f.write(f"# Renderer Response v{new_version}\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}\n")
            f.write(f"**æ¨¡å‹**: {model}\n\n")
            f.write("---\n\n")
            f.write(result)
        print(f"   âœ“ å“åº”å·²ä¿å­˜: {response_file}")

        # æå–å¹¶ä¿å­˜å›¾åƒ
        if "data:image" in result and "base64," in result:
            import re

            match = re.search(r'data:image/[^;]+;base64,([A-Za-z0-9+/=]+)', result)
            if match:
                try:
                    image_data = base64.b64decode(match.group(1))
                    diagram_file = version_dir / "diagram.jpg"
                    with open(diagram_file, "wb") as f:
                        f.write(image_data)

                    # æ›´æ–°æœ€æ–°ç‰ˆæœ¬å·
                    state.set_latest_version(new_version)

                    print(f"   âœ“ å›¾åƒå·²ä¿å­˜: {diagram_file}")
                    print(f"      ç‰ˆæœ¬: v{new_version}")
                    print(f"      æ–‡ä»¶å¤§å°: {len(image_data) / 1024:.2f} KB")
                    return True
                except Exception as e:
                    print(f"âš ï¸  Base64 è§£ç å¤±è´¥: {e}", file=sys.stderr)

        print("âš ï¸  å“åº”ä¸­æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
        return False

    except ImportError:
        print("âŒ ç¼ºå°‘ openai æ¨¡å—ï¼Œè¯·è¿è¡Œ: pip3 install openai", file=sys.stderr)
        return False
    except Exception as e:
        print(f"âŒ Renderer è°ƒç”¨å¼‚å¸¸: {e}", file=sys.stderr)
        return False


# =============================================================================
# ä¸“å®¶å®¡é˜…ç›¸å…³å‡½æ•°
# =============================================================================

def load_review_template(expert_type: str) -> str:
    """åŠ è½½ä¸“å®¶å®¡é˜… prompt æ¨¡æ¿

    Args:
        expert_type: "codex" | "gemini" | "claude"
    """
    template_map = {
        "codex": "review_codex_technical.txt",
        "gemini": "review_gemini_design.txt",
        "claude": "review_claude_academic.txt",
    }
    template_file = Path(__file__).parent.parent / "templates" / template_map.get(expert_type, "")

    if template_file.exists():
        with open(template_file, 'r', encoding='utf-8') as f:
            return f.read()

    print(f"âš ï¸  æœªæ‰¾åˆ°å®¡é˜…æ¨¡æ¿: {template_file}", file=sys.stderr)
    return ""


def run_expert_review(state: TaskState, version: int) -> bool:
    """é˜¶æ®µ3.5: è¿è¡Œä¸“å®¶å®¡é˜…ï¼ˆè°ƒç”¨ parallel-agentï¼‰

    æ–‡ä»¶ç³»ç»ŸçŠ¶æ€æœºï¼š
    - è¾“å…¥: versions/v{N}/diagram.jpg
    - è¾“å‡º: versions/v{N}/review/{expert}_review.md, consensus.json
    """
    print(f"\nğŸ” [é˜¶æ®µ3.5] ä¸“å®¶å®¡é˜… v{version}...")

    version_dir = state.get_version_dir(version)
    review_dir = state.get_version_review_dir(version)
    review_dir.mkdir(parents=True, exist_ok=True)

    diagram_path = version_dir / "diagram.jpg"
    if not diagram_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°æ¶æ„å›¾: {diagram_path}", file=sys.stderr)
        return False

    # åŠ è½½ Visual Schema ä½œä¸ºå‚è€ƒ
    visual_schema = ""
    if state.visual_schema.exists():
        with open(state.visual_schema, 'r', encoding='utf-8') as f:
            visual_schema = f.read()

    # æ„å»º parallel-agent ä»»åŠ¡
    experts = [
        ("codex", "codex", "gpt-5.1-codex-max"),
        ("gemini", "gemini", "gemini-3-pro-preview"),
        ("claude", "claude", "claude-opus-4-5-20251101"),
    ]

    tasks_yaml_parts = []
    for expert_id, backend, model in experts:
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if state.review_stage_complete(version, expert_id):
            print(f"   â­ï¸  è·³è¿‡ {expert_id}ï¼ˆå·²å­˜åœ¨ï¼‰")
            continue

        # åŠ è½½æ¨¡æ¿å¹¶å¡«å……
        template = load_review_template(expert_id)
        prompt = template.replace("{visual_schema}", visual_schema)

        # ä¿å­˜ prompt
        prompt_file = review_dir / f"{expert_id}_prompt.md"
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write(prompt)

        tasks_yaml_parts.append(f"""---TASK---
id: {expert_id}_review
backend: {backend}
model: {model}
images: {diagram_path}
workdir: {state.task_dir}
---CONTENT---
{prompt}
""")

    if not tasks_yaml_parts:
        print("   âœ… æ‰€æœ‰ä¸“å®¶å®¡é˜…å·²å®Œæˆ")
    else:
        # ä¿å­˜ä»»åŠ¡å®šä¹‰
        tasks_yaml = "\n".join(tasks_yaml_parts)
        tasks_file = review_dir / "parallel_tasks.yaml"
        with open(tasks_file, 'w', encoding='utf-8') as f:
            f.write(f"# Auto-generated for version v{version} review\n")
            f.write(f"# Task ID: {state.task_id}\n")
            f.write(f"# Generated: {datetime.now().isoformat()}\n\n")
            f.write(tasks_yaml)
        print(f"   âœ“ ä»»åŠ¡å®šä¹‰: {tasks_file}")

        # è°ƒç”¨ parallel-agent
        parallel_agent_script = Path(__file__).parent.parent.parent / "parallel-agent" / "scripts" / "skill.py"

        if not parallel_agent_script.exists():
            print(f"âŒ æœªæ‰¾åˆ° parallel-agent: {parallel_agent_script}", file=sys.stderr)
            return False

        try:
            print(f"   ğŸš€ è°ƒç”¨ parallel-agent æ‰§è¡Œ {len(tasks_yaml_parts)} ä¸ªå®¡é˜…ä»»åŠ¡...")

            result = subprocess.run(
                ["python3", str(parallel_agent_script)],
                input=tasks_yaml,
                capture_output=True,
                text=True,
                timeout=600  # 10 åˆ†é’Ÿè¶…æ—¶
            )

            # è§£æç»“æœå¹¶ä¿å­˜åˆ°å„ä¸“å®¶æ–‡ä»¶
            output = result.stdout

            # ç®€å•è§£æï¼šæŒ‰ä»»åŠ¡ ID åˆ†å‰²ç»“æœ
            for expert_id, _, _ in experts:
                if state.review_stage_complete(version, expert_id):
                    continue

                # æå–è¯¥ä¸“å®¶çš„è¾“å‡ºï¼ˆç®€åŒ–å¤„ç†ï¼‰
                marker = f"--- Task: {expert_id}_review ---"
                if marker in output:
                    start = output.find(marker)
                    end = output.find("--- Task:", start + len(marker))
                    if end == -1:
                        end = len(output)
                    expert_output = output[start:end].strip()
                else:
                    expert_output = f"[å®¡é˜…ç»“æœ]\n\n{output[:2000]}"

                review_file = review_dir / f"{expert_id}_review.md"
                with open(review_file, 'w', encoding='utf-8') as f:
                    f.write(f"# {expert_id.capitalize()} Expert Review\n\n")
                    f.write(f"**Version**: v{version}\n")
                    f.write(f"**Timestamp**: {datetime.now().isoformat()}\n\n")
                    f.write("---\n\n")
                    f.write(expert_output)

                print(f"   âœ“ {expert_id}_review.md")

            if result.returncode != 0:
                print(f"âš ï¸  parallel-agent è¿”å›éé›¶çŠ¶æ€: {result.returncode}")
                print(f"   stderr: {result.stderr[:500] if result.stderr else 'N/A'}")

        except subprocess.TimeoutExpired:
            print("âŒ parallel-agent æ‰§è¡Œè¶…æ—¶", file=sys.stderr)
            return False
        except Exception as e:
            print(f"âŒ parallel-agent æ‰§è¡Œå¤±è´¥: {e}", file=sys.stderr)
            return False

    # ç”Ÿæˆå…±è¯†ï¼ˆå¦‚æœæ‰€æœ‰å®¡é˜…å®Œæˆï¼‰
    if not state.review_stage_complete(version, "consensus"):
        return generate_consensus(state, version)

    return True


def generate_consensus(state: TaskState, version: int) -> bool:
    """ç”Ÿæˆå…±è¯†ç»“æœ

    æ–‡ä»¶ç³»ç»ŸçŠ¶æ€æœºï¼š
    - è¾“å…¥: versions/v{N}/review/{expert}_review.md
    - è¾“å‡º: versions/v{N}/review/consensus.json
    """
    print(f"   ğŸ“Š ç”Ÿæˆå…±è¯†...")

    review_dir = state.get_version_review_dir(version)

    # è¯»å–æ‰€æœ‰å®¡é˜…ç»“æœ
    reviews = {}
    scores = {}
    all_issues = []

    for expert_id in ["codex", "gemini", "claude"]:
        review_file = review_dir / f"{expert_id}_review.md"
        if review_file.exists():
            with open(review_file, 'r', encoding='utf-8') as f:
                content = f.read()
                reviews[expert_id] = content

                # å°è¯•æå–è¯„åˆ†ï¼ˆç®€å•æ­£åˆ™åŒ¹é…ï¼‰
                import re
                score_match = re.search(r'"score"\s*:\s*(\d+)', content)
                if score_match:
                    scores[expert_id] = int(score_match.group(1))
                else:
                    # å¤‡é€‰ï¼šæŸ¥æ‰¾ X/10 æ ¼å¼
                    score_match = re.search(r'(\d+)/10', content)
                    if score_match:
                        scores[expert_id] = int(score_match.group(1))
                    else:
                        scores[expert_id] = 7  # é»˜è®¤åˆ†æ•°

                # æå–é—®é¢˜ï¼ˆç®€å•åŒ¹é… "issue" å­—æ®µï¼‰
                issue_matches = re.findall(r'"issue"\s*:\s*"([^"]+)"', content)
                for issue in issue_matches:
                    all_issues.append({
                        "issue": issue,
                        "mentioned_by": [expert_id],
                    })

    # åˆå¹¶é‡å¤é—®é¢˜
    merged_issues = []
    for issue in all_issues:
        found = False
        for merged in merged_issues:
            # ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆåŒ…å«å…³ç³»ï¼‰
            if issue["issue"].lower() in merged["issue"].lower() or merged["issue"].lower() in issue["issue"].lower():
                merged["mentioned_by"].extend(issue["mentioned_by"])
                found = True
                break
        if not found:
            merged_issues.append(issue)

    # æ ‡è®°ä¼˜å…ˆçº§
    for issue in merged_issues:
        issue["mentioned_by"] = list(set(issue["mentioned_by"]))
        issue["priority"] = "high" if len(issue["mentioned_by"]) >= 2 else "medium"

    # è®¡ç®—å¹³å‡åˆ†
    avg_score = sum(scores.values()) / len(scores) if scores else 0

    consensus = {
        "version": version,
        "timestamp": datetime.now().isoformat(),
        "scores": scores,
        "avg_score": round(avg_score, 2),
        "issues": sorted(merged_issues, key=lambda x: -len(x["mentioned_by"])),
        "suggestions": [i["issue"] for i in merged_issues[:5]],  # Top 5 å»ºè®®
    }

    # ä¿å­˜å…±è¯†
    consensus_file = review_dir / "consensus.json"
    with open(consensus_file, 'w', encoding='utf-8') as f:
        json.dump(consensus, f, ensure_ascii=False, indent=2)

    print(f"   âœ“ consensus.json (avg: {avg_score:.1f}/10)")

    # æ˜¾ç¤ºå„ä¸“å®¶è¯„åˆ†
    for expert_id, score in scores.items():
        print(f"      [{expert_id}]: {score}/10")

    return True


def load_feedback_synthesis_template() -> str:
    """åŠ è½½åé¦ˆåˆæˆæ¨¡æ¿"""
    template_path = Path(__file__).parent.parent / "templates" / "feedback_synthesis.txt"

    if template_path.exists():
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()

    # é»˜è®¤æ¨¡æ¿
    return """Convert these expert issues into actionable image editing instructions:
{issues_text}

Output specific visual changes (e.g., "Move label X to position Y", "Change font size to Z")."""


def synthesize_feedback_with_llm(state: TaskState, version: int, consensus: Dict[str, Any]) -> str:
    """ä½¿ç”¨ LLM å°†ä¸“å®¶åé¦ˆåˆæˆä¸ºç»“æ„åŒ–ä¿®æ”¹æŒ‡ä»¤

    æ–‡ä»¶ç³»ç»ŸçŠ¶æ€æœºï¼š
    - è¾“å…¥: versions/v{N}/review/{expert}_review.md, consensus.json
    - è¾“å‡º: versions/v{N}/review/synthesized_feedback.md
    """
    review_dir = state.get_version_review_dir(version)

    # åŠ è½½æ¨¡æ¿
    template = load_feedback_synthesis_template()

    # è¯»å–ä¸“å®¶å®¡é˜…åŸæ–‡
    expert_issues = {}
    for expert_id in ["codex", "gemini", "claude"]:
        review_file = review_dir / f"{expert_id}_review.md"
        if review_file.exists():
            with open(review_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # æå–é—®é¢˜éƒ¨åˆ†ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                expert_issues[expert_id] = content[-2000:] if len(content) > 2000 else content

    # æ„å»º prompt
    scores = consensus.get("scores", {})
    issues = consensus.get("issues", [])

    # æ ¼å¼åŒ–ä¸“å®¶é—®é¢˜
    def format_expert_issues(expert_id: str) -> str:
        if expert_id not in expert_issues:
            return "No issues reported."
        # æå– JSON ä¸­çš„ issues éƒ¨åˆ†
        content = expert_issues[expert_id]
        import re
        issue_matches = re.findall(r'"issue"\s*:\s*"([^"]+)"', content)
        if issue_matches:
            return "\n".join(f"- {issue}" for issue in issue_matches[:5])
        return "No structured issues found."

    synthesis_prompt = template.format(
        codex_score=scores.get("codex", "N/A"),
        codex_issues=format_expert_issues("codex"),
        gemini_score=scores.get("gemini", "N/A"),
        gemini_issues=format_expert_issues("gemini"),
        claude_score=scores.get("claude", "N/A"),
        claude_issues=format_expert_issues("claude"),
        avg_score=consensus.get("avg_score", 0),
        high_priority_count=len([i for i in issues if i.get("priority") == "high"]),
        total_issues=len(issues),
    )

    # ä¿å­˜åˆæˆ prompt
    synthesis_prompt_file = review_dir / "synthesis_prompt.md"
    with open(synthesis_prompt_file, 'w', encoding='utf-8') as f:
        f.write(synthesis_prompt)

    try:
        print(f"   ğŸ¤– è°ƒç”¨ LLM åˆæˆåé¦ˆ...")

        result = subprocess.run(
            ["codex", "exec", "-"],
            input=synthesis_prompt,
            capture_output=True,
            text=True,
            timeout=120
        )

        if result.returncode == 0 and result.stdout.strip():
            synthesized = result.stdout.strip()

            # ä¿å­˜åˆæˆç»“æœ
            feedback_file = review_dir / "synthesized_feedback.md"
            with open(feedback_file, 'w', encoding='utf-8') as f:
                f.write(f"# Synthesized Feedback v{version}\n\n")
                f.write(f"**Generated**: {datetime.now().isoformat()}\n\n")
                f.write("---\n\n")
                f.write(synthesized)

            print(f"   âœ“ synthesized_feedback.md ({len(synthesized)} chars)")
            return synthesized
        else:
            print(f"   âš ï¸  LLM åˆæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åé¦ˆ")

    except subprocess.TimeoutExpired:
        print(f"   âš ï¸  LLM åˆæˆè¶…æ—¶ï¼Œä½¿ç”¨ç®€å•åé¦ˆ")
    except Exception as e:
        print(f"   âš ï¸  LLM åˆæˆå¼‚å¸¸: {e}")

    # å›é€€åˆ°ç®€å•åé¦ˆ
    return None


def make_iteration_decision(state: TaskState, version: int, threshold: int = 8) -> bool:
    """é˜¶æ®µ3.6: ç”Ÿæˆè¿­ä»£å†³ç­–

    æ–‡ä»¶ç³»ç»ŸçŠ¶æ€æœºï¼š
    - è¾“å…¥: versions/v{N}/review/consensus.json
    - è¾“å‡º: versions/v{N}/review/iteration_decision.json, synthesized_feedback.md
    """
    print(f"   ğŸ¯ ç”Ÿæˆè¿­ä»£å†³ç­–...")

    review_dir = state.get_version_review_dir(version)

    # åŠ è½½å…±è¯†
    consensus = state.load_consensus(version)
    if not consensus:
        print("âŒ æœªæ‰¾åˆ°å…±è¯†ç»“æœ", file=sys.stderr)
        return False

    avg_score = consensus.get("avg_score", 0)
    issues = consensus.get("issues", [])
    high_priority_issues = [i for i in issues if i.get("priority") == "high"]

    # å†³ç­–é€»è¾‘
    should_iterate = avg_score < threshold and len(issues) > 0

    generated_feedback = ""
    if should_iterate:
        reason = f"avg_score ({avg_score:.1f}) < threshold ({threshold})"
        if high_priority_issues:
            reason += f", æœ‰ {len(high_priority_issues)} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜"

        # å°è¯•ä½¿ç”¨ LLM åˆæˆç»“æ„åŒ–åé¦ˆ
        synthesized = synthesize_feedback_with_llm(state, version, consensus)

        if synthesized:
            generated_feedback = synthesized
        else:
            # å›é€€ï¼šç®€å•åˆ—è¡¨æ ¼å¼
            feedback_parts = [
                "## Refinement Instructions for Imagen 3\n",
                "### PRIORITY FIXES"
            ]
            for i, issue in enumerate(issues[:5], 1):
                priority_mark = "**[HIGH]**" if issue.get("priority") == "high" else "[medium]"
                mentioned = ", ".join(issue.get("mentioned_by", []))
                feedback_parts.append(f"{i}. {priority_mark} {issue['issue']} (mentioned by: {mentioned})")

            feedback_parts.append("\n### STYLE GUIDANCE")
            feedback_parts.append("- Maintain CVPR academic standard")
            feedback_parts.append("- Use mathematical notation for labels")
            feedback_parts.append("- Keep existing layout structure")

            generated_feedback = "\n".join(feedback_parts)

            # ä¿å­˜ç®€å•åé¦ˆ
            feedback_file = review_dir / "synthesized_feedback.md"
            with open(feedback_file, 'w', encoding='utf-8') as f:
                f.write(f"# Simple Feedback v{version}\n\n")
                f.write(f"**Generated**: {datetime.now().isoformat()}\n")
                f.write(f"**Mode**: Fallback (LLM unavailable)\n\n")
                f.write("---\n\n")
                f.write(generated_feedback)
    else:
        reason = f"avg_score ({avg_score:.1f}) >= threshold ({threshold})" if avg_score >= threshold else "æ— æ”¹è¿›å»ºè®®"

    decision = {
        "version": version,
        "timestamp": datetime.now().isoformat(),
        "avg_score": avg_score,
        "threshold": threshold,
        "should_iterate": should_iterate,
        "reason": reason,
        "generated_feedback": generated_feedback,
        "feedback_method": "llm_synthesized" if should_iterate and synthesize_feedback_with_llm else "simple_list",
    }

    # ä¿å­˜å†³ç­–
    decision_file = review_dir / "iteration_decision.json"
    with open(decision_file, 'w', encoding='utf-8') as f:
        json.dump(decision, f, ensure_ascii=False, indent=2)

    if should_iterate:
        print(f"   â†’ éœ€è¦è¿­ä»£ ({reason})")
    else:
        print(f"   âœ… å®¡é˜…é€šè¿‡ ({reason})")

    return True


def extract_and_propose_lessons(state: TaskState, version: int, learn_mode: bool = False) -> List[Dict[str, Any]]:
    """ä»å®¡é˜…ä¸­æå–æ–° Lessonsï¼ˆåŒæ­¥æ¨¡å¼ - è¿”å›å¾…å®¡æ‰¹åˆ—è¡¨ä¾›å¤–éƒ¨ç¡®è®¤ï¼‰

    Args:
        state: ä»»åŠ¡çŠ¶æ€
        version: ç‰ˆæœ¬å·
        learn_mode: æ˜¯å¦å¯ç”¨å­¦ä¹ æ¨¡å¼

    Returns:
        æå–çš„å¾…å®¡æ‰¹ Lesson åˆ—è¡¨ï¼ˆä¾›å¤–éƒ¨å®¡æ‰¹æµç¨‹ä½¿ç”¨ï¼‰
    """
    if not learn_mode:
        return []

    consensus = state.load_consensus(version)
    if not consensus:
        return []

    # æå–æ–°çš„å¾…å®¡æ‰¹ Lessons
    pending_lessons = extract_new_lessons_from_review(consensus, {})

    if not pending_lessons:
        print("   ğŸ“ æœªå‘ç°æ–°çš„é«˜ä¼˜å…ˆçº§é—®é¢˜")
        return []

    print(f"\n   ğŸ“ å‘ç° {len(pending_lessons)} æ¡æ–°é—®é¢˜æ¨¡å¼:")
    for i, lesson in enumerate(pending_lessons, 1):
        print(f"      [{i}] {lesson['title']}")
        print(f"          ç±»åˆ«: {lesson.get('category', 'unknown')}")
        print(f"          æåŠ: {', '.join(lesson.get('mentioned_by', []))}")

    # è¾“å‡º JSON åˆ°çŠ¶æ€ç›®å½•ä¾›å¤–éƒ¨å®¡æ‰¹
    pending_file = state.versions_dir / f"v{version}" / "pending_lessons.json"
    pending_file.parent.mkdir(parents=True, exist_ok=True)
    with open(pending_file, 'w', encoding='utf-8') as f:
        json.dump(pending_lessons, f, ensure_ascii=False, indent=2)
    print(f"\n   ğŸ’¾ å¾…å®¡æ‰¹åˆ—è¡¨å·²ä¿å­˜: {pending_file}")
    print(f"   ğŸ’¡ ç¡®è®¤åè°ƒç”¨ approve_lesson_to_spec() å°† Lesson å†™å…¥ Spec")

    return pending_lessons


# =============================================================================
# Pipeline ä¸»æµç¨‹
# =============================================================================

def run_pipeline(state: TaskState, args: argparse.Namespace, force: bool = False, iterate: bool = False):
    """è¿è¡Œå®Œæ•´ pipelineï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è¿­ä»£å’Œè‡ªåŠ¨å®¡é˜…

    è‡ªåŠ¨å®¡é˜…æ¨¡å¼ (--auto-review):
    - æ¸²æŸ“åè‡ªåŠ¨è°ƒç”¨ä¸‰ä½ä¸“å®¶å®¡é˜…
    - åŸºäºå…±è¯†åˆ†æ•°å†³å®šæ˜¯å¦ç»§ç»­è¿­ä»£
    - è¾¾åˆ°é˜ˆå€¼æˆ–æœ€å¤§è¿­ä»£æ¬¡æ•°ååœæ­¢
    """

    if force:
        print("\nâš ï¸  å¼ºåˆ¶æ¨¡å¼ï¼šæ¸…é™¤æ‰€æœ‰å·²æœ‰æ–‡ä»¶")
        state.clear()

    # è¿ç§»æ—§ç‰ˆæ–‡ä»¶
    state.migrate_legacy_diagram()

    state.print_status()

    # è·å–è‡ªåŠ¨å®¡é˜…å‚æ•°
    auto_review = getattr(args, 'auto_review', False)
    max_iterations = getattr(args, 'max_iterations', 5)
    review_threshold = getattr(args, 'review_threshold', 8)

    # è¿­ä»£æ¨¡å¼ï¼šåªé‡æ–°æ¸²æŸ“
    if iterate and args.feedback:
        print("\nğŸ”„ è¿­ä»£æ¨¡å¼ï¼šåŸºäºä¸Šä¸€ç‰ˆæœ¬ + åé¦ˆç”Ÿæˆæ–°ç‰ˆæœ¬")

        if not state.stage_complete("schema"):
            print("âŒ è¿­ä»£æ¨¡å¼éœ€è¦å·²æœ‰ Visual Schema", file=sys.stderr)
            return False

        if not run_renderer(state, feedback=args.feedback, extra_reference_images=args.reference_images):
            print("\nâŒ è¿­ä»£æ¸²æŸ“å¤±è´¥")
            state.print_status()
            return False

        # å¦‚æœå¯ç”¨è‡ªåŠ¨å®¡é˜…ï¼Œå¯¹æ–°ç‰ˆæœ¬è¿›è¡Œå®¡é˜…
        if auto_review:
            version = state.get_latest_version()
            if not run_expert_review(state, version):
                print("âš ï¸  ä¸“å®¶å®¡é˜…å¤±è´¥ï¼Œä½†å›¾åƒå·²ç”Ÿæˆ")
            else:
                make_iteration_decision(state, version, threshold=review_threshold)
                # æå–æ–° Lessonsï¼ˆå¦‚æœå¯ç”¨å­¦ä¹ æ¨¡å¼ï¼‰
                learn_mode = getattr(args, 'learn', False)
                extract_and_propose_lessons(state, version, learn_mode=learn_mode)

        print("\n" + "=" * 50)
        print("ğŸ‰ è¿­ä»£å®Œæˆ!")
        print("=" * 50)
        state.print_status()
        return True

    # é˜¶æ®µ0: ä¿å­˜è¾“å…¥
    if not state.stage_complete("input"):
        save_input(state, args)
    else:
        print(f"\nâ­ï¸  è·³è¿‡é˜¶æ®µ0ï¼ˆå·²å­˜åœ¨: {state.input_json.name}ï¼‰")

    # é˜¶æ®µ1: ä»£ç åˆ†æ
    if not state.stage_complete("analysis"):
        if not run_analysis(state):
            print("\nâŒ é˜¶æ®µ1å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            state.print_status()
            return False
    else:
        print(f"\nâ­ï¸  è·³è¿‡é˜¶æ®µ1ï¼ˆå·²å­˜åœ¨: {state.analysis_md.name}ï¼‰")

    # é˜¶æ®µ2: ç”Ÿæˆ Visual Schema
    if not state.stage_complete("schema"):
        if not run_architect(state):
            print("\nâŒ é˜¶æ®µ2å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            state.print_status()
            return False
    else:
        print(f"\nâ­ï¸  è·³è¿‡é˜¶æ®µ2ï¼ˆå·²å­˜åœ¨: {state.visual_schema.name}ï¼‰")

    # é˜¶æ®µ3: æ¸²æŸ“å›¾åƒ
    if not state.stage_complete("diagram"):
        if not run_renderer(state, extra_reference_images=args.reference_images):
            print("\nâŒ é˜¶æ®µ3å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            state.print_status()
            return False
    else:
        print(f"\nâ­ï¸  è·³è¿‡é˜¶æ®µ3ï¼ˆå·²æœ‰ç‰ˆæœ¬: v{state.get_latest_version()}ï¼‰")
        print("   ğŸ’¡ ä½¿ç”¨ --iterate --feedback \"ä¿®æ”¹éœ€æ±‚\" è¿›è¡Œè¿­ä»£")

    # é˜¶æ®µ3.5-4: è‡ªåŠ¨å®¡é˜…å¾ªç¯
    if auto_review:
        print("\n" + "=" * 50)
        print("ğŸ”„ è‡ªåŠ¨å®¡é˜…æ¨¡å¼å¯åŠ¨")
        print(f"   é˜ˆå€¼: {review_threshold}/10 | æœ€å¤§è¿­ä»£: {max_iterations}")
        print("=" * 50)

        iteration_count = 0
        while iteration_count < max_iterations:
            version = state.get_latest_version()
            print(f"\nğŸ“ å®¡é˜…å¾ªç¯ #{iteration_count + 1} (v{version})")

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥ç‰ˆæœ¬çš„å®¡é˜…
            if state.review_stage_complete(version, "decision"):
                # åŠ è½½å·²æœ‰å†³ç­–
                decision = state.load_iteration_decision(version)
                if decision and not decision.get("should_iterate", False):
                    print(f"   âœ… v{version} å·²é€šè¿‡å®¡é˜… (avg: {decision.get('avg_score', 'N/A')}/10)")
                    break
                elif decision and decision.get("should_iterate"):
                    # ä½¿ç”¨å·²æœ‰åé¦ˆç»§ç»­è¿­ä»£
                    feedback = decision.get("generated_feedback", "")
                    if feedback:
                        print(f"   ğŸ“ ä½¿ç”¨å·²æœ‰åé¦ˆç»§ç»­è¿­ä»£...")
                        if not run_renderer(state, feedback=feedback, extra_reference_images=args.reference_images):
                            print("âŒ è¿­ä»£æ¸²æŸ“å¤±è´¥")
                            break
                        iteration_count += 1
                        continue
            else:
                # æ‰§è¡Œä¸“å®¶å®¡é˜…
                if not run_expert_review(state, version):
                    print("âŒ ä¸“å®¶å®¡é˜…å¤±è´¥")
                    break

                # ç”Ÿæˆè¿­ä»£å†³ç­–
                if not make_iteration_decision(state, version, threshold=review_threshold):
                    print("âŒ å†³ç­–ç”Ÿæˆå¤±è´¥")
                    break

                # æå–æ–° Lessonsï¼ˆå¦‚æœå¯ç”¨å­¦ä¹ æ¨¡å¼ï¼‰
                learn_mode = getattr(args, 'learn', False)
                extract_and_propose_lessons(state, version, learn_mode=learn_mode)

            # åŠ è½½æ–°å†³ç­–
            decision = state.load_iteration_decision(version)
            if not decision:
                print("âŒ æ— æ³•åŠ è½½å†³ç­–ç»“æœ")
                break

            if not decision.get("should_iterate", False):
                print(f"\nâœ… å®¡é˜…é€šè¿‡ï¼æœ€ç»ˆç‰ˆæœ¬: v{version}")
                break

            # éœ€è¦è¿­ä»£
            feedback = decision.get("generated_feedback", "")
            if not feedback:
                print("âš ï¸  éœ€è¦è¿­ä»£ä½†æ— åé¦ˆï¼Œåœæ­¢å¾ªç¯")
                break

            print(f"\nğŸ”„ æ‰§è¡Œè‡ªåŠ¨è¿­ä»£ (v{version} â†’ v{version + 1})...")
            if not run_renderer(state, feedback=feedback, extra_reference_images=args.reference_images):
                print("âŒ è¿­ä»£æ¸²æŸ“å¤±è´¥")
                break

            iteration_count += 1

        if iteration_count >= max_iterations:
            print(f"\nâš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})")

        print("\n" + "=" * 50)
        print(f"ğŸ” è‡ªåŠ¨å®¡é˜…å®Œæˆ (è¿­ä»£ {iteration_count} æ¬¡)")
        print("=" * 50)

    print("\n" + "=" * 50)
    print("ğŸ‰ æ¶æ„å›¾ç”Ÿæˆå®Œæˆ!")
    print("=" * 50)
    state.print_status()

    return True


def list_tasks(base_dir: str = DEFAULT_BASE_DIR):
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡åŠçŠ¶æ€"""
    base_path = Path(base_dir)

    if not base_path.exists():
        print(f"ğŸ“‚ ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return

    tasks = [d for d in base_path.iterdir() if d.is_dir()]

    if not tasks:
        print(f"ğŸ“‚ æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡")
        return

    print(f"\nğŸ“‚ ä»»åŠ¡åˆ—è¡¨ ({base_dir})")
    print("=" * 70)

    for task_dir in sorted(tasks):
        state = TaskState(task_dir.name, base_dir)
        status = state.get_status()
        latest_version = status["latest_version"]

        # çŠ¶æ€å›¾æ ‡
        if status["has_diagram"]:
            icon = "âœ…"
        elif status["schema"]:
            icon = "ğŸ”¶"
        elif status["analysis"]:
            icon = "ğŸ”·"
        else:
            icon = "â¬œ"

        version_str = f"v{latest_version}" if latest_version > 0 else "v0"
        print(f"{icon} {task_dir.name:<40} [{version_str}]")

    print("=" * 70)
    print("å›¾ä¾‹: âœ… æœ‰æ¶æ„å›¾ | ğŸ”¶ Schemaå·²ç”Ÿæˆ | ğŸ”· åˆ†æå·²å®Œæˆ | â¬œ åˆšå¼€å§‹")


def generate_task_id(arch_code_path: Optional[str] = None) -> str:
    """ç”Ÿæˆä»»åŠ¡ ID"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if arch_code_path:
        basename = Path(arch_code_path).stem
        return f"{basename}_{timestamp}"
    else:
        return f"task_{timestamp}"


# =============================================================================
# ä¸»å…¥å£
# =============================================================================

def main():
    model_config = load_model_config()

    parser = argparse.ArgumentParser(
        description="å­¦æœ¯æ¶æ„å›¾ç”Ÿæˆå™¨ - æ–‡ä»¶ç³»ç»Ÿå³çŠ¶æ€æœºè®¾è®¡ v2.3ï¼ˆæ”¯æŒæ¸è¿›å¼è¿­ä»£ + è‡ªåŠ¨ä¸“å®¶å®¡é˜…ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ–°å»ºä»»åŠ¡
  python3 skill.py --arch_code_path basicofr/archs/freqmamba_arch.py

  # æ¢å¤ä»»åŠ¡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
  python3 skill.py --resume freqmamba_arch_20251231_160000

  # æ¸è¿›å¼è¿­ä»£ï¼ˆåŸºäºä¸Šä¸€ç‰ˆæœ¬ + åé¦ˆï¼‰
  python3 skill.py --resume task_id --iterate --feedback "ä¿®æ”¹æ–‡å­—æ ‡æ³¨ä¸ºæ•°å­¦ç¬¦å·é£æ ¼"

  # å¯ç”¨è‡ªåŠ¨ä¸“å®¶å®¡é˜…ï¼ˆä¸‰ä¸“å®¶å¹¶è¡Œå®¡é˜…ï¼Œè‡ªåŠ¨è¿­ä»£ç›´åˆ°é€šè¿‡ï¼‰
  python3 skill.py --arch_code_path arch.py --auto-review

  # è‡ªå®šä¹‰å®¡é˜…å‚æ•°
  python3 skill.py --resume task_id --auto-review --review-threshold 7 --max-iterations 3

  # åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡
  python3 skill.py --list

  # å¼ºåˆ¶é‡æ–°æ‰§è¡Œ
  python3 skill.py --resume freqmamba_arch_20251231_160000 --force
        """
    )

    # è¾“å…¥å‚æ•°
    parser.add_argument(
        "--arch_code_path",
        type=str,
        help="æ¶æ„ä»£ç è·¯å¾„ï¼ˆä»£ç åˆ†ææ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--paper_content",
        type=str,
        help="è®ºæ–‡å†…å®¹/æ–¹æ³•æè¿°ï¼ˆè®ºæ–‡æ¨¡å¼ï¼‰"
    )

    # ä»»åŠ¡ç®¡ç†
    parser.add_argument(
        "--resume",
        type=str,
        metavar="TASK_ID",
        help="æ¢å¤å·²æœ‰ä»»åŠ¡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡åŠçŠ¶æ€"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°æ‰§è¡Œæ‰€æœ‰é˜¶æ®µ"
    )

    # è¿­ä»£å‚æ•°
    parser.add_argument(
        "--iterate",
        action="store_true",
        help="è¿­ä»£æ¨¡å¼ï¼šåŸºäºä¸Šä¸€ç‰ˆæœ¬ç”Ÿæˆæ–°ç‰ˆæœ¬"
    )
    parser.add_argument(
        "--feedback",
        type=str,
        help="è¿­ä»£åé¦ˆ/ä¿®æ”¹éœ€æ±‚ï¼ˆä¸ --iterate é…åˆä½¿ç”¨ï¼‰"
    )

    # è‡ªåŠ¨å®¡é˜…å‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    parser.add_argument(
        "--auto-review",
        dest="auto_review",
        action="store_true",
        default=True,
        help="å¯ç”¨è‡ªåŠ¨ä¸“å®¶å®¡é˜…æ¨¡å¼ï¼ˆCodex/Gemini/Claude ä¸‰ä¸“å®¶å¹¶è¡Œå®¡é˜…ï¼‰[é»˜è®¤å¯ç”¨]"
    )
    parser.add_argument(
        "--no-auto-review",
        dest="auto_review",
        action="store_false",
        help="ç¦ç”¨è‡ªåŠ¨ä¸“å®¶å®¡é˜…æ¨¡å¼"
    )
    parser.add_argument(
        "--max-iterations",
        dest="max_iterations",
        type=int,
        default=5,
        help="è‡ªåŠ¨å®¡é˜…æœ€å¤§è¿­ä»£æ¬¡æ•° (é»˜è®¤: 5)"
    )
    parser.add_argument(
        "--review-threshold",
        dest="review_threshold",
        type=int,
        default=8,
        help="å®¡é˜…é€šè¿‡é˜ˆå€¼ï¼Œ1-10åˆ† (é»˜è®¤: 8)"
    )
    parser.add_argument(
        "--learn",
        action="store_true",
        help="å¯ç”¨å­¦ä¹ æ¨¡å¼ï¼šä»å®¡é˜…ä¸­æå–æ–°é—®é¢˜åˆ° Spec å¾…å®¡æ‰¹åŒºï¼ˆéœ€æ‰‹åŠ¨å®¡æ‰¹åç”Ÿæ•ˆï¼‰"
    )
    parser.add_argument(
        "--approve-lessons",
        type=str,
        help="å®¡æ‰¹æŒ‡å®šçš„ Lessonsï¼ˆé€—å·åˆ†éš”çš„ç´¢å¼•ï¼Œå¦‚ '1,2,3' æˆ– 'all'ï¼‰ã€‚éœ€é…åˆ --task-id ä½¿ç”¨ã€‚"
    )
    parser.add_argument(
        "--task-id",
        type=str,
        help="æŒ‡å®šä»»åŠ¡ IDï¼ˆç”¨äºå®¡æ‰¹ Lessons æ—¶å®šä½ pending_lessons.jsonï¼‰"
    )

    # è¾“å‡ºé…ç½®
    parser.add_argument(
        "--output_path",
        type=str,
        default=DEFAULT_BASE_DIR,
        help=f"è¾“å‡ºåŸºç¡€ç›®å½• (é»˜è®¤: {DEFAULT_BASE_DIR})"
    )

    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--model_architect",
        type=str,
        default=model_config['architect'],
        help=f"Architect æ¨¡å‹ (é»˜è®¤: {model_config['architect']})"
    )
    parser.add_argument(
        "--model_renderer",
        type=str,
        default=model_config['renderer'],
        help=f"Renderer æ¨¡å‹ (é»˜è®¤: {model_config['renderer']})"
    )
    parser.add_argument(
        "--reference_images",
        type=str,
        nargs='+',
        help="å‚è€ƒå›¾åƒè·¯å¾„ï¼ˆæ”¯æŒå¤šå¼ ï¼‰"
    )

    args = parser.parse_args()

    # åˆ—å‡ºä»»åŠ¡æ¨¡å¼
    if args.list:
        list_tasks(args.output_path)
        return

    # å®¡æ‰¹ Lessons æ¨¡å¼
    if getattr(args, 'approve_lessons', None):
        task_id = getattr(args, 'task_id', None)
        if not task_id:
            print("âŒ é”™è¯¯: --approve-lessons éœ€è¦é…åˆ --task-id ä½¿ç”¨", file=sys.stderr)
            sys.exit(1)

        # æŸ¥æ‰¾ pending_lessons.json
        state = TaskState(task_id, args.output_path)
        # æ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬çš„ pending_lessons.json
        pending_file = None
        if state.versions_dir.exists():
            versions = sorted(state.versions_dir.glob("v*"))
            for v in reversed(versions):
                pf = v / "pending_lessons.json"
                if pf.exists():
                    pending_file = pf
                    break

        if not pending_file or not pending_file.exists():
            print(f"âŒ æœªæ‰¾åˆ°å¾…å®¡æ‰¹åˆ—è¡¨: {task_id}", file=sys.stderr)
            print("   è¯·å…ˆè¿è¡Œ --auto-review --learn æ¨¡å¼ç”Ÿæˆå¾…å®¡æ‰¹åˆ—è¡¨", file=sys.stderr)
            sys.exit(1)

        with open(pending_file, 'r', encoding='utf-8') as f:
            pending_lessons = json.load(f)

        if not pending_lessons:
            print("ğŸ“ æ²¡æœ‰å¾…å®¡æ‰¹çš„ Lessons")
            return

        # è§£æè¦å®¡æ‰¹çš„ç´¢å¼•
        approve_arg = args.approve_lessons.strip().lower()
        if approve_arg == 'all':
            indices = list(range(len(pending_lessons)))
        else:
            try:
                indices = [int(i.strip()) - 1 for i in approve_arg.split(',')]
            except ValueError:
                print("âŒ æ— æ•ˆçš„ç´¢å¼•æ ¼å¼ï¼Œè¯·ä½¿ç”¨ '1,2,3' æˆ– 'all'", file=sys.stderr)
                sys.exit(1)

        # å®¡æ‰¹é€‰ä¸­çš„ Lessons
        approved_count = 0
        print(f"\nğŸ“‹ å®¡æ‰¹ Lessons (ä»»åŠ¡: {task_id}):\n")
        for idx in indices:
            if 0 <= idx < len(pending_lessons):
                lesson = pending_lessons[idx]
                if approve_lesson_to_spec(lesson, task_id):
                    approved_count += 1
            else:
                print(f"   âš ï¸  ç´¢å¼• {idx + 1} è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡", file=sys.stderr)

        print(f"\nâœ… å·²å®¡æ‰¹ {approved_count}/{len(indices)} æ¡ Lessons")
        print(f"   ä¸‹æ¬¡è¿è¡Œ arch-diagram æ—¶å°†è‡ªåŠ¨åº”ç”¨è¿™äº›ç»éªŒ")
        return

    # ç¡®å®šä»»åŠ¡ ID
    if args.resume:
        task_id = args.resume
        print(f"\nğŸ“‚ æ¢å¤ä»»åŠ¡: {task_id}")
    elif args.arch_code_path or args.paper_content:
        task_id = generate_task_id(args.arch_code_path)
        print(f"\nğŸ“‚ æ–°å»ºä»»åŠ¡: {task_id}")
    else:
        print("âŒ é”™è¯¯: å¿…é¡»æä¾› --arch_code_pathã€--paper_content æˆ– --resume ä¹‹ä¸€", file=sys.stderr)
        parser.print_help()
        sys.exit(1)

    # åˆ›å»ºçŠ¶æ€ç®¡ç†å™¨
    state = TaskState(task_id, args.output_path)

    # æ¢å¤æ¨¡å¼ä¸‹ï¼Œä»å·²ä¿å­˜çš„è¾“å…¥åŠ è½½å‚æ•°
    if args.resume and state.input_json.exists():
        saved_input = state.load_input()
        if saved_input:
            # ä½¿ç”¨ä¿å­˜çš„å‚æ•°ï¼Œä½†å…è®¸å‘½ä»¤è¡Œè¦†ç›–
            if not args.arch_code_path:
                args.arch_code_path = saved_input.get('arch_code_path')
            if not args.paper_content:
                args.paper_content = saved_input.get('paper_content')
            if args.model_architect == model_config['architect']:
                args.model_architect = saved_input.get('model_architect', args.model_architect)
            if args.model_renderer == model_config['renderer']:
                args.model_renderer = saved_input.get('model_renderer', args.model_renderer)
            # æ³¨æ„ï¼šreference_images ä¸ä»ä¿å­˜çš„è¾“å…¥åŠ è½½ï¼Œå…è®¸æ¯æ¬¡è¿­ä»£ä½¿ç”¨ä¸åŒå‚è€ƒå›¾

    # è¿è¡Œ pipeline
    success = run_pipeline(state, args, force=args.force, iterate=args.iterate)

    if not success:
        print("\nğŸ’¡ æç¤º: ä¿®å¤é—®é¢˜åï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç»§ç»­:")
        print(f"   python3 skill.py --resume {task_id}")
        sys.exit(1)


if __name__ == "__main__":
    main()