#!/usr/bin/env python3
"""
å°†è®ºæ–‡ PDF è½¬æ¢ä¸º Markdownï¼ˆåŸºäº marker-pdfï¼‰ã€‚

ä¾èµ–ï¼š
  - pip install marker-pdf
  - pip install requests  # arXiv ä¸‹è½½ç”¨
  - codex CLIï¼ˆç”¨äºç”Ÿæˆè§„èŒƒçš„æ–‡ä»¶å¤¹åç§°ï¼‰

ç”¨æ³•ï¼š
  python3 paper2markdown.py /path/to/paper.pdf
  python3 paper2markdown.py 2301.12345
  python3 paper2markdown.py arxiv:2301.12345
  python3 paper2markdown.py /path/to/paper.pdf --out-dir ./output --overwrite
"""

from __future__ import annotations

import argparse
import json
import re
import shutil
import subprocess
import sys
import tempfile
from datetime import datetime
from pathlib import Path

try:
    import requests
except ImportError:
    requests = None

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None


def _eprint(msg: str) -> None:
    sys.stderr.write(msg + "\n")


def _log(msg: str) -> None:
    if tqdm is not None:
        tqdm.write(msg, file=sys.stderr)
    else:
        _eprint(msg)


def _parse_arxiv_id(value: str) -> str | None:
    """è§£æ arXiv IDï¼Œæ”¯æŒ 2301.12345ã€arxiv:2301.12345ã€2301.12345v2 æ ¼å¼"""
    m = re.match(r"^(arxiv:)?(\d{4}\.\d{4,5})(v\d+)?$", value.strip())
    if not m:
        return None
    arxiv_id = m.group(2)
    version = m.group(3) or ""
    return f"{arxiv_id}{version}"


def _download_arxiv_pdf(arxiv_id: str) -> Path:
    """ä¸‹è½½ arXiv PDF åˆ°ä¸´æ—¶ç›®å½•"""
    if requests is None:
        raise RuntimeError("ç¼ºå°‘ä¾èµ– requestsï¼šè¯·å…ˆ pip install requests")

    url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
    dest = Path(tempfile.gettempdir()) / f"arxiv_{arxiv_id}.pdf"

    _log(f"ğŸ“¥ ä¸‹è½½ arXiv è®ºæ–‡: {url}")
    try:
        with requests.get(url, stream=True, timeout=(10, 120)) as resp:
            resp.raise_for_status()
            total = resp.headers.get("Content-Length")
            total_bytes = int(total) if total and total.isdigit() else None
            pbar = (
                tqdm(
                    total=total_bytes,
                    unit="B",
                    unit_scale=True,
                    unit_divisor=1024,
                    desc="Download",
                    file=sys.stderr,
                )
                if tqdm is not None
                else None
            )
            with dest.open("wb") as f:
                for chunk in resp.iter_content(chunk_size=1024 * 1024):
                    if not chunk:
                        continue
                    f.write(chunk)
                    if pbar is not None:
                        pbar.update(len(chunk))
            if pbar is not None:
                pbar.close()
        _log(f"âœ… ä¸‹è½½å®Œæˆ: {dest}")
    except requests.exceptions.Timeout as exc:
        _cleanup_file(dest)
        raise RuntimeError(f"ç½‘ç»œè¶…æ—¶ï¼š{url}") from exc
    except requests.exceptions.RequestException as exc:
        _cleanup_file(dest)
        raise RuntimeError(f"ä¸‹è½½å¤±è´¥ï¼š{url}ï¼ˆ{exc}ï¼‰") from exc
    except OSError as exc:
        _cleanup_file(dest)
        raise RuntimeError(f"å†™å…¥å¤±è´¥ï¼š{dest}ï¼ˆ{exc}ï¼‰") from exc

    return dest


def _cleanup_file(path: Path) -> None:
    """å®‰å…¨åˆ é™¤æ–‡ä»¶"""
    try:
        if path and path.exists():
            path.unlink()
    except Exception:
        pass


def _fix_image_paths(md_content: str) -> str:
    """ä¿®å¤ markdown ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼Œæ·»åŠ  ./ å‰ç¼€ä»¥å…¼å®¹æ›´å¤šæŸ¥çœ‹å™¨"""
    # åŒ¹é… ![alt](path) æ ¼å¼ï¼Œå…¶ä¸­ path ä¸ä»¥ http/https/./ å¼€å¤´
    # ä¾‹å¦‚ ![](_page_1_Figure_2.jpeg) -> ![](Paper/_page_1_Figure_2.jpeg)
    import re

    def fix_path(match):
        alt = match.group(1)
        path = match.group(2)
        # è·³è¿‡å·²ç»æ˜¯ç»å¯¹è·¯å¾„æˆ– http(s) é“¾æ¥çš„æƒ…å†µ
        if path.startswith(('http://', 'https://', './', '../', '/')):
            return match.group(0)
        # æ·»åŠ  ./ å‰ç¼€
        return f'![{alt}](./{path})'

    return re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', fix_path, md_content)


def _run_marker(pdf_path: Path, output_dir: Path, page_range: str | None = None) -> Path:
    """ä½¿ç”¨ marker_single è½¬æ¢ PDF ä¸º Markdown"""
    _log(f"ğŸ”„ è½¬æ¢ PDF: {pdf_path.name}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)

    cmd = [
        "marker_single",
        "--output_format", "markdown",
        "--output_dir", str(output_dir),
    ]
    if page_range:
        cmd.extend(["--page_range", page_range])
    cmd.append(str(pdf_path))

    try:
        _log(f"ğŸ”§ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        _log("ğŸ“Š marker å¤„ç†è¿›åº¦ï¼ˆç”± marker_single è¾“å‡ºï¼‰ï¼š")
        result = subprocess.run(cmd, timeout=1800)  # 30 åˆ†é’Ÿè¶…æ—¶
        if result.returncode != 0:
            raise RuntimeError(f"marker_single æ‰§è¡Œå¤±è´¥: {result.returncode}")
    except FileNotFoundError:
        raise RuntimeError("marker_single å‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£…: pip install marker-pdf")
    except subprocess.TimeoutExpired:
        raise RuntimeError("marker_single æ‰§è¡Œè¶…æ—¶ï¼ˆ>30åˆ†é’Ÿï¼‰")

    # marker_single è¾“å‡ºç»“æ„: output_dir/pdf_stem/pdf_stem/pdf_stem.md
    # æˆ–è€…: output_dir/pdf_stem/pdf_stem.md
    pdf_stem = pdf_path.stem
    possible_paths = [
        output_dir / pdf_stem / pdf_stem / f"{pdf_stem}.md",
        output_dir / pdf_stem / f"{pdf_stem}.md",
        output_dir / f"{pdf_stem}.md",
    ]

    for p in possible_paths:
        if p.exists():
            _log(f"âœ… è½¬æ¢å®Œæˆ: {p}")
            return p

    # å°è¯•æŸ¥æ‰¾ä»»ä½• .md æ–‡ä»¶
    md_files = list(output_dir.rglob("*.md"))
    if md_files:
        _log(f"âœ… è½¬æ¢å®Œæˆ: {md_files[0]}")
        return md_files[0]

    raise RuntimeError(f"è½¬æ¢åæœªæ‰¾åˆ° Markdown æ–‡ä»¶ï¼Œæ£€æŸ¥ç›®å½•: {output_dir}")


def _generate_folder_name(md_content: str, fallback_name: str) -> str:
    """ä½¿ç”¨ Codex ç”Ÿæˆç¬¦åˆè§„èŒƒçš„æ–‡ä»¶å¤¹åç§°"""
    # å®šä½ codex skill è„šæœ¬
    codex_script = Path(__file__).parent.parent.parent / "codex" / "scripts" / "codex.py"

    if not codex_script.exists():
        _log(f"âš ï¸ Codex skill è„šæœ¬ä¸å­˜åœ¨: {codex_script}")
        return _extract_title_fallback(md_content, fallback_name)

    # å–å‰ 3000 å­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
    content_preview = md_content[:3000]

    prompt = f'''æ ¹æ®ä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ CamelCase æ ¼å¼çš„æ–‡ä»¶å¤¹åç§°ã€‚

è¦æ±‚ï¼š
1. åç§°åº”è¯¥åæ˜ è®ºæ–‡çš„æ ¸å¿ƒæ–¹æ³•æˆ–åˆ›æ–°ç‚¹ï¼ˆå¦‚æ–¹æ³•åã€æ¶æ„åï¼‰
2. ä½¿ç”¨ CamelCase æ ¼å¼ï¼ˆå¦‚ MambaIRã€WaveMambaã€DefMambaã€FreqMambaã€WaveletMaskï¼‰
3. é•¿åº¦æ§åˆ¶åœ¨ 5-20 ä¸ªå­—ç¬¦
4. åªè¾“å‡ºåç§°æœ¬èº«ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€æ ‡ç‚¹æˆ–æ¢è¡Œ

å‚è€ƒå·²æœ‰å‘½åï¼šDefMambaã€FreqMambaã€MambaIRã€WaveletMaskã€WaveMambaã€MemFlowã€SCSA

è®ºæ–‡å†…å®¹ï¼š
{content_preview}

è¾“å‡ºï¼ˆä»…ä¸€ä¸ª CamelCase åç§°ï¼‰ï¼š'''

    try:
        result = subprocess.run(
            [sys.executable, str(codex_script), prompt, "-r", "low"],
            capture_output=True,
            text=True,
            timeout=60
        )
        if result.returncode == 0 and result.stdout.strip():
            # è§£æè¾“å‡ºï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ CamelCase åç§°
            output = result.stdout.strip()
            # å°è¯•ä»è¾“å‡ºä¸­æå–ç¬¦åˆæ ¼å¼çš„åç§°
            for line in output.split('\n'):
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œè§£é‡Šæ€§æ–‡å­—
                if not line or line.startswith('#') or ':' in line:
                    continue
                # æ¸…ç†å¯èƒ½çš„æ ‡ç‚¹
                name = re.sub(r'[^\w]', '', line)
                # éªŒè¯åç§°æ ¼å¼
                if re.match(r'^[A-Z][a-zA-Z0-9]{2,25}$', name):
                    _log(f"ğŸ“ Codex ç”Ÿæˆåç§°: {name}")
                    return name

            _log(f"âš ï¸ Codex è¾“å‡ºæ— æ•ˆ: {output[:100]}ï¼Œä½¿ç”¨ fallback")
    except FileNotFoundError:
        _log("âš ï¸ codex è„šæœ¬æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ fallback åç§°")
    except subprocess.TimeoutExpired:
        _log("âš ï¸ codex æ‰§è¡Œè¶…æ—¶ï¼Œä½¿ç”¨ fallback åç§°")
    except Exception as e:
        _log(f"âš ï¸ codex æ‰§è¡Œå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback åç§°")

    # Fallback: ä» markdown æå–æ ‡é¢˜
    return _extract_title_fallback(md_content, fallback_name)


def _extract_title_fallback(md_content: str, fallback_name: str) -> str:
    """ä» markdown å†…å®¹æå–æ ‡é¢˜ä½œä¸º fallback"""
    # å°è¯•åŒ¹é… # æ ‡é¢˜
    match = re.search(r'^#\s+(.+)$', md_content, re.MULTILINE)
    if match:
        title = match.group(1).strip()
        # æå–å…³é”®è¯ç”Ÿæˆ CamelCase
        words = re.findall(r'[A-Z][a-z]+|[A-Z]+(?=[A-Z]|$)|[a-z]+', title)
        if words:
            # å–å‰ 3 ä¸ªæœ‰æ„ä¹‰çš„è¯
            meaningful = [w for w in words if len(w) > 2][:3]
            if meaningful:
                name = ''.join(w.capitalize() for w in meaningful)
                if 3 <= len(name) <= 25:
                    return name

    # æœ€ç»ˆ fallback
    return _sanitize_name(fallback_name)


def _sanitize_name(name: str) -> str:
    """æ¸…ç†åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    # ç§»é™¤ç‰ˆæœ¬å·
    name = re.sub(r'v\d+$', '', name)
    # ç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦
    name = re.sub(r'[^a-zA-Z0-9]', '', name)
    # ç¡®ä¿é¦–å­—æ¯å¤§å†™
    if name and name[0].islower():
        name = name[0].upper() + name[1:]
    return name[:25] if name else "Paper"


def _copy_tree_with_progress(src_dir: Path, dest_dir: Path) -> int:
    """é€’å½’å¤åˆ¶ç›®å½•ï¼Œå¹¶åœ¨æ–‡ä»¶çº§åˆ«æ˜¾ç¤ºè¿›åº¦æ¡ã€‚"""
    if not src_dir.exists():
        return 0

    dest_dir.mkdir(parents=True, exist_ok=True)
    files = [p for p in src_dir.rglob("*") if p.is_file()]
    pbar = (
        tqdm(total=len(files), desc=f"Copy {src_dir.name}", unit="file", file=sys.stderr)
        if tqdm is not None and files
        else None
    )

    copied = 0
    for file_path in files:
        rel = file_path.relative_to(src_dir)
        target = dest_dir / rel
        target.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(file_path, target)
        copied += 1
        if pbar is not None:
            pbar.update(1)

    if pbar is not None:
        pbar.close()
    return copied


# ============== Phase 2: ä»£ç ä¸‹è½½ç›¸å…³å‡½æ•° ==============

def _extract_github_url(md_content: str) -> str | None:
    """ä»è®ºæ–‡æ‘˜è¦éƒ¨åˆ†æå– GitHub URL"""
    # èšç„¦å‰ 3000 å­—ç¬¦ï¼ˆé€šå¸¸åŒ…å« Abstractï¼‰
    abstract_region = md_content[:3000]

    # åŒ¹é… GitHub URL
    match = re.search(r'https?://github\.com/([\w.-]+)/([\w.-]+)', abstract_region)
    if match:
        url = f"https://github.com/{match.group(1)}/{match.group(2)}"
        # æ¸…ç† URL æœ«å°¾å¯èƒ½çš„æ ‡ç‚¹
        url = re.sub(r'[.,;:)\]]+$', '', url)
        return url

    return None


def _get_repo_tree(github_url: str) -> str | None:
    """è·å–ä»“åº“ Python æ–‡ä»¶åˆ—è¡¨"""
    if requests is None:
        _log("âš ï¸ ç¼ºå°‘ requests åº“ï¼Œæ— æ³•è·å–ä»“åº“ç»“æ„")
        return None

    match = re.match(r'https?://github\.com/([\w.-]+)/([\w.-]+)', github_url)
    if not match:
        return None

    owner, repo = match.groups()

    # å°è¯• main å’Œ master åˆ†æ”¯
    for branch in ['main', 'master']:
        api_url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/{branch}?recursive=1"
        try:
            resp = requests.get(api_url, timeout=30)
            if resp.status_code == 200:
                tree = resp.json().get('tree', [])
                # åªä¿ç•™ .py æ–‡ä»¶ï¼Œæ’é™¤æ— å…³ç›®å½•
                py_files = [
                    item['path'] for item in tree
                    if item['path'].endswith('.py')
                    and not any(x in item['path'].lower() for x in
                               ['test/', 'tests/', 'docs/', 'examples/', 'scripts/', 'demo/'])
                ]
                if py_files:
                    return '\n'.join(py_files)
        except Exception as e:
            _log(f"âš ï¸ è·å–ä»“åº“ç»“æ„å¤±è´¥ ({branch}): {e}")
            continue

    return None


def _identify_core_files(repo_tree: str, paper_title: str) -> list[str]:
    """ä½¿ç”¨ Codex è¯†åˆ«æ ¸å¿ƒæ¶æ„æ–‡ä»¶"""
    # å®šä½ codex skill è„šæœ¬
    codex_script = Path(__file__).parent.parent.parent / "codex" / "scripts" / "codex.py"

    if not codex_script.exists():
        _log(f"âš ï¸ Codex skill è„šæœ¬ä¸å­˜åœ¨: {codex_script}")
        return _fallback_identify_core_files(repo_tree)

    prompt = f'''ä½ æ˜¯æ·±åº¦å­¦ä¹ ä»£ç åˆ†æä¸“å®¶ã€‚åˆ†æä»¥ä¸‹ GitHub ä»“åº“ç»“æ„ï¼Œè¯†åˆ«å®ç°è®ºæ–‡æ ¸å¿ƒåˆ›æ–°çš„æ¶æ„æ–‡ä»¶ã€‚

## è®ºæ–‡æ ‡é¢˜
{paper_title}

## ä»“åº“ Python æ–‡ä»¶åˆ—è¡¨
{repo_tree}

## ä»»åŠ¡
è¯†åˆ« 1-3 ä¸ªæ ¸å¿ƒç½‘ç»œæ¶æ„æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶åº”è¯¥ï¼š
1. åŒ…å« nn.Module å­ç±»å®šä¹‰
2. å®ç°è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ï¼ˆå¦‚æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ã€ç½‘ç»œç»“æ„ï¼‰
3. æ–‡ä»¶åé€šå¸¸åŒ…å« archã€netã€modelã€networkã€formerã€mamba ç­‰å…³é”®è¯

## æ’é™¤
- __init__.py
- train.py, test.py, inference.pyï¼ˆè®­ç»ƒ/æµ‹è¯•è„šæœ¬ï¼‰
- losses.py, loss.py, metrics.pyï¼ˆæŸå¤±/æŒ‡æ ‡ï¼‰
- data*.py, dataset*.pyï¼ˆæ•°æ®åŠ è½½ï¼‰
- utils.py, tools.py, helpers.pyï¼ˆå·¥å…·å‡½æ•°ï¼‰
- base_model.py, base_arch.pyï¼ˆåŸºç±»ï¼‰
- options/, configs/ï¼ˆé…ç½®æ–‡ä»¶ï¼‰

## è¾“å‡ºæ ¼å¼
åªè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ï¼š'''

    try:
        result = subprocess.run(
            [sys.executable, str(codex_script), prompt, "-r", "low"],
            capture_output=True,
            text=True,
            timeout=120
        )

        if result.returncode == 0 and result.stdout.strip():
            # è§£æè¾“å‡ºï¼Œæå–æœ‰æ•ˆè·¯å¾„
            lines = result.stdout.strip().split('\n')
            paths = []
            for line in lines:
                line = line.strip()
                # è¿‡æ»¤æœ‰æ•ˆçš„ Python æ–‡ä»¶è·¯å¾„
                if (line.endswith('.py')
                    and '/' in line
                    and not line.startswith('#')
                    and not line.startswith('-')
                    and '__init__' not in line):
                    paths.append(line)

            if paths:
                _log(f"ğŸ¤– Codex è¯†åˆ«åˆ° {len(paths[:3])} ä¸ªæ ¸å¿ƒæ–‡ä»¶")
                return paths[:3]

        _log("âš ï¸ Codex æœªè¿”å›æœ‰æ•ˆç»“æœï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…")
    except subprocess.TimeoutExpired:
        _log("âš ï¸ Codex æ‰§è¡Œè¶…æ—¶ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…")
    except Exception as e:
        _log(f"âš ï¸ Codex æ‰§è¡Œå¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…")

    return _fallback_identify_core_files(repo_tree)


def _fallback_identify_core_files(repo_tree: str) -> list[str]:
    """è§„åˆ™åŒ¹é…è¯†åˆ«æ ¸å¿ƒæ–‡ä»¶ï¼ˆfallbackï¼‰"""
    files = repo_tree.strip().split('\n')

    # ä¼˜å…ˆçº§å…³é”®è¯
    priority_keywords = ['arch', 'network', 'net', 'model', 'former', 'mamba', 'attention']
    exclude_keywords = ['__init__', 'base', 'utils', 'tools', 'train', 'test', 'loss',
                       'data', 'config', 'option', 'inference', 'demo']

    candidates = []
    for f in files:
        f_lower = f.lower()
        # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶
        if any(ex in f_lower for ex in exclude_keywords):
            continue
        # æ£€æŸ¥ä¼˜å…ˆçº§å…³é”®è¯
        score = sum(1 for kw in priority_keywords if kw in f_lower)
        if score > 0:
            candidates.append((score, f))

    # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰ 3 ä¸ª
    candidates.sort(key=lambda x: -x[0])
    result = [f for _, f in candidates[:3]]

    if result:
        _log(f"ğŸ“‹ è§„åˆ™åŒ¹é…è¯†åˆ«åˆ° {len(result)} ä¸ªæ ¸å¿ƒæ–‡ä»¶")

    return result


def _download_code_files(github_url: str, file_paths: list[str], dest_dir: Path) -> tuple[list[str], dict[str, str]]:
    """ä¸‹è½½ä»£ç æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•

    Returns:
        (å·²ä¸‹è½½çš„æ–‡ä»¶ååˆ—è¡¨, æ–‡ä»¶ååˆ°åŸå§‹è·¯å¾„çš„æ˜ å°„)
    """
    if requests is None:
        _log("âš ï¸ ç¼ºå°‘ requests åº“ï¼Œæ— æ³•ä¸‹è½½ä»£ç ")
        return [], {}

    match = re.match(r'https?://github\.com/([\w.-]+)/([\w.-]+)', github_url)
    if not match:
        return [], {}

    owner, repo = match.groups()
    dest_dir.mkdir(parents=True, exist_ok=True)

    downloaded = []
    file_mapping = {}  # è®°å½•åŸå§‹è·¯å¾„åˆ°æ–‡ä»¶åçš„æ˜ å°„

    for file_path in file_paths:
        # å°è¯• main å’Œ master åˆ†æ”¯
        success = False
        for branch in ['main', 'master']:
            raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{file_path}"
            try:
                resp = requests.get(raw_url, timeout=30)
                if resp.status_code == 200:
                    filename = Path(file_path).name
                    dest_file = dest_dir / filename
                    dest_file.write_text(resp.text, encoding='utf-8')
                    downloaded.append(filename)
                    file_mapping[filename] = file_path
                    _log(f"âœ… ä¸‹è½½: {filename}")
                    success = True
                    break
            except Exception as e:
                continue

        if not success:
            _log(f"âš ï¸ ä¸‹è½½å¤±è´¥: {file_path}")

    # ä¿å­˜æ¥æºå…ƒæ•°æ®
    if downloaded:
        meta = {
            "source": github_url,
            "branch": "main",  # é»˜è®¤è®°å½• main
            "files": file_mapping,
            "downloaded_at": datetime.now().isoformat()
        }
        meta_file = dest_dir / "_source.json"
        meta_file.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding='utf-8')
        _log(f"ğŸ“ ä¿å­˜å…ƒæ•°æ®: {meta_file.name}")

    return downloaded, file_mapping


def _parse_local_imports(code_content: str, repo_files: set[str]) -> list[str]:
    """è§£æä»£ç ä¸­çš„æœ¬åœ°å¯¼å…¥ï¼Œè¿”å›éœ€è¦ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„

    Args:
        code_content: Python ä»£ç å†…å®¹
        repo_files: ä»“åº“ä¸­æ‰€æœ‰ Python æ–‡ä»¶è·¯å¾„çš„é›†åˆ

    Returns:
        éœ€è¦ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    imports = set()

    # åŒ¹é… from xxx import yyy å’Œ import xxx
    # ä¾‹å¦‚: from models.GBC import GBC -> models/GBC.py
    # ä¾‹å¦‚: from mmcv.cnn.bricks.transformer import build_dropout -> mmcv/cnn/bricks/transformer.py
    patterns = [
        r'^from\s+([\w.]+)\s+import',  # from xxx import
        r'^import\s+([\w.]+)',          # import xxx
    ]

    for line in code_content.split('\n'):
        line = line.strip()
        if not line or line.startswith('#'):
            continue

        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                module_path = match.group(1)
                # è½¬æ¢ä¸ºæ–‡ä»¶è·¯å¾„: models.GBC -> models/GBC.py
                file_path = module_path.replace('.', '/') + '.py'
                imports.add(file_path)
                # ä¹Ÿå°è¯• __init__.py
                init_path = module_path.replace('.', '/') + '/__init__.py'
                imports.add(init_path)
                break

    # è¿‡æ»¤ï¼šåªä¿ç•™ä»“åº“ä¸­å­˜åœ¨çš„æ–‡ä»¶
    result = []
    for imp in imports:
        # æ£€æŸ¥å®Œæ•´è·¯å¾„æˆ–éƒ¨åˆ†åŒ¹é…
        for repo_file in repo_files:
            if repo_file.endswith(imp) or imp in repo_file:
                result.append(repo_file)
                break

    return list(set(result))


def _resolve_dependencies(
    dest_dir: Path,
    github_url: str,
    repo_tree: str,
    downloaded: list[str],
    file_mapping: dict[str, str],
    max_depth: int = 2
) -> tuple[list[str], dict[str, str]]:
    """é€’å½’è§£æå¹¶ä¸‹è½½ä¾èµ–

    Args:
        dest_dir: ç›®æ ‡ç›®å½•
        github_url: GitHub ä»“åº“ URL
        repo_tree: ä»“åº“æ–‡ä»¶æ ‘ï¼ˆæ¢è¡Œåˆ†éš”çš„è·¯å¾„ï¼‰
        downloaded: å·²ä¸‹è½½çš„æ–‡ä»¶ååˆ—è¡¨
        file_mapping: æ–‡ä»¶ååˆ°åŸå§‹è·¯å¾„çš„æ˜ å°„
        max_depth: æœ€å¤§é€’å½’æ·±åº¦

    Returns:
        (æ›´æ–°åçš„ downloaded åˆ—è¡¨, æ›´æ–°åçš„ file_mapping)
    """
    if max_depth <= 0:
        return downloaded, file_mapping

    # æ„å»ºä»“åº“æ–‡ä»¶é›†åˆ
    repo_files = set(repo_tree.strip().split('\n'))

    # è§£æ GitHub URL
    match = re.match(r'https?://github\.com/([\w.-]+)/([\w.-]+)', github_url)
    if not match:
        return downloaded, file_mapping

    owner, repo = match.groups()

    # éå†å·²ä¸‹è½½çš„æ–‡ä»¶ï¼Œè§£æä¾èµ–
    new_deps = []
    for filename in downloaded:
        file_path = dest_dir / filename
        if not file_path.exists() or not filename.endswith('.py'):
            continue

        try:
            code_content = file_path.read_text(encoding='utf-8')
        except Exception:
            continue

        # è§£ææœ¬åœ°å¯¼å…¥
        deps = _parse_local_imports(code_content, repo_files)
        for dep in deps:
            dep_name = Path(dep).name
            # è·³è¿‡å·²ä¸‹è½½çš„
            if dep_name in downloaded or dep_name in [Path(d).name for d in new_deps]:
                continue
            # è·³è¿‡ __init__.py
            if dep_name == '__init__.py':
                continue
            new_deps.append(dep)

    if not new_deps:
        return downloaded, file_mapping

    _log(f"ğŸ”— å‘ç° {len(new_deps)} ä¸ªä¾èµ–æ–‡ä»¶")

    # ä¸‹è½½æ–°ä¾èµ–
    new_downloaded = []
    for dep_path in new_deps:
        for branch in ['main', 'master']:
            raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{dep_path}"
            try:
                resp = requests.get(raw_url, timeout=30)
                if resp.status_code == 200:
                    filename = Path(dep_path).name
                    dest_file = dest_dir / filename
                    dest_file.write_text(resp.text, encoding='utf-8')
                    new_downloaded.append(filename)
                    file_mapping[filename] = dep_path
                    _log(f"  âœ… ä¾èµ–: {filename}")
                    break
            except Exception:
                continue

    # æ›´æ–°å·²ä¸‹è½½åˆ—è¡¨
    downloaded = downloaded + new_downloaded

    # é€’å½’å¤„ç†æ–°ä¸‹è½½çš„ä¾èµ–
    if new_downloaded and max_depth > 1:
        return _resolve_dependencies(
            dest_dir, github_url, repo_tree,
            downloaded, file_mapping, max_depth - 1
        )

    return downloaded, file_mapping


# ============== ä»£ç æœ¬åœ°åŒ–ï¼ˆCodex å¤„ç†ï¼‰==============

def _codex_localize_code(codes_dir: Path, downloaded: list[str], paper_name: str) -> None:
    """ä½¿ç”¨ Codex æœ¬åœ°åŒ–ä»£ç ï¼Œç§»é™¤ mmcv/mmengine ç­‰å¤§å‹æ¡†æ¶ä¾èµ–

    Args:
        codes_dir: ä»£ç ç›®å½•
        downloaded: å·²ä¸‹è½½çš„æ–‡ä»¶ååˆ—è¡¨
        paper_name: è®ºæ–‡/é¡¹ç›®åç§°
    """
    if not downloaded:
        return

    # å®šä½ codex skill è„šæœ¬
    codex_script = Path(__file__).parent.parent.parent / "codex" / "scripts" / "codex.py"
    if not codex_script.exists():
        _log("âš ï¸ Codex skill ä¸å¯ç”¨ï¼Œè·³è¿‡ä»£ç æœ¬åœ°åŒ–")
        return

    # æ„å»ºæ–‡ä»¶åˆ—è¡¨
    py_files = [f for f in downloaded if f.endswith('.py')]
    if not py_files:
        return

    files_list = '\n'.join(f'- {codes_dir / f}' for f in py_files)

    prompt = f'''ä½ æ˜¯æ·±åº¦å­¦ä¹ ä»£ç è¿ç§»ä¸“å®¶ã€‚è¯·æœ¬åœ°åŒ–ä»¥ä¸‹ä»£ç æ–‡ä»¶ï¼Œä½¿å…¶ä¸ä¾èµ– mmcv/mmengine/mmcls ç­‰å¤§å‹æ¡†æ¶ã€‚

## ç›®æ ‡é¡¹ç›®
{paper_name}

## å¾…å¤„ç†æ–‡ä»¶
{files_list}

## æœ¬åœ°åŒ–åŸåˆ™

1. **ç§»é™¤å¤§å‹æ¡†æ¶ä¾èµ–**ï¼š
   - mmcv â†’ ç”¨ PyTorch/timm åŸç”Ÿå®ç°æ›¿ä»£
   - mmengine â†’ ç”¨ PyTorch åŸç”Ÿå®ç°æ›¿ä»£
   - mmcls/mmseg/mmdet â†’ ç§»é™¤æ³¨å†Œå™¨ï¼Œç›´æ¥ä½¿ç”¨ç±»

2. **ä¿ç•™å…è®¸çš„ä¾èµ–**ï¼š
   - torch, torchvisionï¼ˆæ ¸å¿ƒï¼‰
   - timmï¼ˆé¢„è®­ç»ƒæ¨¡å‹ã€DropPath ç­‰ï¼‰
   - einopsï¼ˆå¼ é‡æ“ä½œï¼‰
   - mamba_ssm, selective_scan_cudaï¼ˆMamba ç›¸å…³ï¼‰
   - basicsrï¼ˆè®­ç»ƒæ¡†æ¶ï¼‰

3. **æ›¿æ¢ç­–ç•¥**ï¼š
   - `from mmcv.runner import BaseModule` â†’ ç»§æ‰¿ `nn.Module`ï¼Œæ·»åŠ  `init_cfg` å‚æ•°
   - `from mmcv.cnn import build_norm_layer` â†’ ç›´æ¥ç”¨ `nn.LayerNorm/BatchNorm2d`
   - `from mmcv.cnn.bricks.transformer import build_dropout` â†’ ç”¨ `timm.models.layers.DropPath`
   - `@BACKBONES.register_module()` â†’ ç§»é™¤è£…é¥°å™¨ï¼ˆæˆ–æ”¹ç”¨ basicsr çš„ @ARCH_REGISTRY.register()ï¼‰
   - `from mmcv.utils import to_2tuple` â†’ ä» timm å¯¼å…¥æˆ–è‡ªå·±å®ç°

4. **ä»£ç è´¨é‡è¦æ±‚**ï¼š
   - ä¿æŒåŸæœ‰åŠŸèƒ½ä¸å˜
   - æ·»åŠ å¿…è¦çš„å¯¼å…¥è¯­å¥
   - å¦‚éœ€è¾…åŠ©å‡½æ•°ï¼Œåˆ›å»º _utils.py
   - ç”Ÿæˆ __init__.py å¯¼å‡ºä¸»è¦ç±»

## è¾“å‡ºè¦æ±‚
ç›´æ¥ä¿®æ”¹æ–‡ä»¶ï¼Œä¸è¦è¾“å‡ºè§£é‡Šã€‚ç¡®ä¿ä¿®æ”¹åçš„ä»£ç å¯ä»¥æ­£å¸¸å¯¼å…¥ã€‚
'''

    try:
        _log("ğŸ¤– Codex æ­£åœ¨æœ¬åœ°åŒ–ä»£ç ...")
        result = subprocess.run(
            [sys.executable, str(codex_script), prompt, "-r", "high"],
            capture_output=True,
            text=True,
            timeout=300,
            cwd=str(codes_dir.parent.parent)  # åœ¨ Ideas/<project> ç›®å½•ä¸‹è¿è¡Œ
        )

        if result.returncode == 0:
            _log("âœ… ä»£ç æœ¬åœ°åŒ–å®Œæˆ")
        else:
            _log(f"âš ï¸ Codex è¿”å›éé›¶çŠ¶æ€: {result.returncode}")
            if result.stderr:
                _log(f"   {result.stderr[:200]}")

    except subprocess.TimeoutExpired:
        _log("âš ï¸ Codex æ‰§è¡Œè¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œè·³è¿‡æœ¬åœ°åŒ–")
    except Exception as e:
        _log(f"âš ï¸ Codex æ‰§è¡Œå¤±è´¥: {e}")


def _codex_normalize_code(codes_dir: Path, folder_name: str, paper_content: str) -> bool:
    """ä½¿ç”¨ Codex å°†åŸå§‹ä»£ç è§„èŒƒåŒ–ï¼Œä¿å­˜åˆ° Codes/ ç›®å½•

    å‚è€ƒ Ideas/DefMamba/DefMamba.py çš„è§„èŒƒæ ¼å¼ï¼š
    - ç§»é™¤ mmcv/mmengine ç­‰å¤§å‹æ¡†æ¶ä¾èµ–
    - æ·»åŠ ä¸­æ–‡æ³¨é‡Šè¯´æ˜æ ¸å¿ƒé€»è¾‘
    - ç”Ÿæˆ <Name>.py + utils.py + __init__.py

    Args:
        codes_dir: ä»£ç ç›®å½• (Codes/)
        folder_name: é¡¹ç›®åç§°ï¼ˆå¦‚ DefMambaï¼‰
        paper_content: è®ºæ–‡ markdown å†…å®¹

    Returns:
        æ˜¯å¦æˆåŠŸç”Ÿæˆè§„èŒƒåŒ–ä»£ç 
    """
    if not codes_dir.exists():
        return False

    # æ”¶é›†æ‰€æœ‰ Python æ–‡ä»¶
    py_files = [f for f in codes_dir.glob("*.py") if not f.name.startswith('_')]
    if not py_files:
        return False

    # å®šä½ codex skill è„šæœ¬
    codex_script = Path(__file__).parent.parent.parent / "codex" / "scripts" / "codex.py"
    if not codex_script.exists():
        _log("âš ï¸ Codex skill ä¸å¯ç”¨ï¼Œè·³è¿‡è§„èŒƒåŒ–ä»£ç ç”Ÿæˆ")
        return False

    # è¯»å–åŸå§‹ä»£ç å†…å®¹
    code_contents = {}
    for py_file in py_files:
        try:
            code_contents[py_file.name] = py_file.read_text(encoding='utf-8')
        except Exception:
            continue

    if not code_contents:
        return False

    # æ„å»ºæ–‡ä»¶åˆ—è¡¨å­—ç¬¦ä¸²ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
    files_info = '\n'.join([f"### {codes_dir}/{name}\n```python\n{content[:4000]}\n```"
                           for name, content in list(code_contents.items())[:3]])

    prompt = f'''ä½ æ˜¯æ·±åº¦å­¦ä¹ ä»£ç è§„èŒƒåŒ–ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹åŸå§‹ä»£ç è½¬æ¢ä¸ºè§„èŒƒæ ¼å¼ã€‚

## é¡¹ç›®åç§°
{folder_name}

## è®ºæ–‡æ‘˜è¦ï¼ˆç”¨äºç†è§£æ ¸å¿ƒåˆ›æ–°ï¼‰
{paper_content[:2500]}

## åŸå§‹ä»£ç æ–‡ä»¶
{files_info}

## è§„èŒƒåŒ–è¦æ±‚

### 1. ä¾èµ–å¤„ç†
ç§»é™¤ä»¥ä¸‹å¤§å‹æ¡†æ¶ä¾èµ–ï¼Œæ›¿æ¢ä¸º PyTorch/timm åŸç”Ÿå®ç°ï¼š
- mmcv, mmengine, mmdet, mmseg, mmcls â†’ ç§»é™¤
- `from mmcv.runner import BaseModule` â†’ ç»§æ‰¿ `nn.Module`
- `from mmcv.cnn import build_norm_layer` â†’ ä½¿ç”¨ `nn.LayerNorm/BatchNorm2d`
- `@BACKBONES.register_module()` â†’ ç§»é™¤è£…é¥°å™¨
- `from mmcv.utils import to_2tuple` â†’ ä» timm å¯¼å…¥æˆ–è‡ªå·±å®ç°

ä¿ç•™å…è®¸çš„ä¾èµ–ï¼š
- torch, torchvision
- timm (DropPath, trunc_normal_ ç­‰)
- einops
- mamba_ssm, selective_scan_cuda

### 2. ä»£ç ç»“æ„ï¼ˆå‚è€ƒ Ideas/DefMamba/DefMamba.pyï¼‰
```python
"""
{folder_name} ä¸»ä½“ç½‘ç»œ

æ ¸å¿ƒæµç¨‹ï¼š
1) xxxï¼šåŠŸèƒ½æè¿°
2) yyyï¼šåŠŸèƒ½æè¿°
"""
import torch
import torch.nn as nn
from timm.layers import DropPath, trunc_normal_
from einops import rearrange

class CoreModule(nn.Module):
    """æ ¸å¿ƒæ¨¡å—è¯´æ˜ï¼ˆä¸­æ–‡ï¼‰"""
    def __init__(self, ...):
        super().__init__()
        # åˆå§‹åŒ–è¯´æ˜

    def forward(self, x):
        # (B, C, H, W) -> (B, C, H, W)
        return x
```

### 3. è¾“å‡ºæ–‡ä»¶ï¼ˆä¿å­˜åˆ° {codes_dir}ï¼‰

1. **{codes_dir}/{folder_name}.py** - ä¸»æ¶æ„æ–‡ä»¶
   - åŒ…å«æ ¸å¿ƒç½‘ç»œç±»
   - æ¯ä¸ªç±»å’Œé‡è¦æ–¹æ³•æœ‰ä¸­æ–‡æ³¨é‡Š
   - å¼ é‡ç»´åº¦åœ¨æ³¨é‡Šä¸­æ ‡æ˜

2. **{codes_dir}/utils.py** - å·¥å…·å‡½æ•°ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰
   - SelectiveScanã€è¾…åŠ©å‡½æ•°ç­‰

3. **{codes_dir}/__init__.py** - æ¨¡å—å¯¼å‡º
   ```python
   """
   {folder_name} - æ ¸å¿ƒåˆ›æ–°ç‚¹ç®€è¿°

   æ ¸å¿ƒæ¨¡å—ï¼š
   - MainClass: ä¸»ç½‘ç»œ
   - HelperClass: è¾…åŠ©æ¨¡å—
   """
   from .{folder_name} import MainClass, HelperClass
   __all__ = ['MainClass', 'HelperClass']
   ```

## è¾“å‡ºè¦æ±‚
ç›´æ¥ä¿®æ”¹/ç”Ÿæˆæ–‡ä»¶åˆ° {codes_dir} ç›®å½•ï¼Œä¸è¦è¾“å‡ºè§£é‡Šã€‚ç¡®ä¿ä»£ç å¯ä»¥æ­£å¸¸å¯¼å…¥ã€‚
'''

    try:
        _log("ğŸ¤– Codex æ­£åœ¨è§„èŒƒåŒ–ä»£ç ...")
        result = subprocess.run(
            [sys.executable, str(codex_script), prompt, "-r", "high"],
            capture_output=True,
            text=True,
            timeout=600,  # 10 åˆ†é’Ÿè¶…æ—¶
            cwd=str(codes_dir.parent)  # åœ¨ Ideas/<Name>/ ç›®å½•ä¸‹è¿è¡Œ
        )

        if result.returncode == 0:
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†è§„èŒƒåŒ–æ–‡ä»¶
            main_file = codes_dir / f"{folder_name}.py"
            init_file = codes_dir / "__init__.py"
            if main_file.exists() or init_file.exists():
                _log(f"âœ… è§„èŒƒåŒ–ä»£ç ç”Ÿæˆå®Œæˆ: {codes_dir}")
                return True
            else:
                _log("âš ï¸ Codex æœªç”Ÿæˆé¢„æœŸæ–‡ä»¶")
        else:
            _log(f"âš ï¸ Codex è¿”å›éé›¶çŠ¶æ€: {result.returncode}")
            if result.stderr:
                _log(f"   {result.stderr[:300]}")

    except subprocess.TimeoutExpired:
        _log("âš ï¸ Codex æ‰§è¡Œè¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰ï¼Œè·³è¿‡è§„èŒƒåŒ–")
    except Exception as e:
        _log(f"âš ï¸ Codex æ‰§è¡Œå¤±è´¥: {e}")

    return False


def _check_existing_paper(base_dir: Path, folder_name: str = None) -> tuple[Path | None, str | None]:
    """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è½¬æ¢å¥½çš„è®ºæ–‡ markdown

    Args:
        base_dir: Ideas åŸºç›®å½•
        folder_name: å¯é€‰çš„æŒ‡å®šæ–‡ä»¶å¤¹å

    Returns:
        (md_path, folder_name) å¦‚æœå­˜åœ¨ï¼Œå¦åˆ™ (None, None)
    """
    if folder_name:
        paper_dir = base_dir / folder_name / "Paper"
        if paper_dir.exists():
            md_files = list(paper_dir.glob("*.md"))
            if md_files:
                return md_files[0], folder_name
    return None, None


def _find_existing_project(base_dir: Path, arxiv_id: str = None, pdf_stem: str = None) -> tuple[Path | None, str | None]:
    """æ ¹æ® arXiv ID æˆ– PDF æ–‡ä»¶åæŸ¥æ‰¾å·²å­˜åœ¨çš„é¡¹ç›®

    Args:
        base_dir: Ideas åŸºç›®å½•
        arxiv_id: arXiv IDï¼ˆå¦‚ 2501.04486ï¼‰
        pdf_stem: PDF æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰

    Returns:
        (md_path, folder_name) å¦‚æœå­˜åœ¨ï¼Œå¦åˆ™ (None, None)
    """
    if not base_dir.exists():
        return None, None

    # éå† Ideas ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    for project_dir in base_dir.iterdir():
        if not project_dir.is_dir():
            continue

        paper_dir = project_dir / "Paper"
        if not paper_dir.exists():
            continue

        # æ£€æŸ¥æ˜¯å¦æœ‰ md æ–‡ä»¶
        md_files = list(paper_dir.glob("*.md"))
        if not md_files:
            continue

        # æ£€æŸ¥ md å†…å®¹æ˜¯å¦åŒ¹é… arXiv ID
        if arxiv_id:
            for md_file in md_files:
                try:
                    content = md_file.read_text(encoding='utf-8')[:2000]
                    if arxiv_id in content or f"arxiv.org/abs/{arxiv_id}" in content.lower():
                        return md_file, project_dir.name
                except Exception:
                    continue

        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…
        if pdf_stem:
            pdf_stem_lower = pdf_stem.lower().replace('_', '').replace('-', '')
            for md_file in md_files:
                md_stem = md_file.stem.lower().replace('_', '').replace('-', '')
                if pdf_stem_lower in md_stem or md_stem in pdf_stem_lower:
                    return md_file, project_dir.name

    return None, None


def main() -> int:
    parser = argparse.ArgumentParser(
        description="Convert paper PDF to Markdown via marker-pdf"
    )
    parser.add_argument(
        "input",
        help="è®ºæ–‡ PDF è·¯å¾„ æˆ– arXiv IDï¼ˆå¦‚ 2301.12345 / arxiv:2301.12345ï¼‰"
    )
    parser.add_argument(
        "--out-dir",
        default=None,
        help="è¾“å‡ºåŸºç›®å½•ï¼ˆé»˜è®¤ï¼š./Ideas/ï¼‰"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶"
    )
    parser.add_argument(
        "--page-range",
        default=None,
        help="é¡µé¢èŒƒå›´ï¼ˆå¦‚ 0-9 è¡¨ç¤ºå‰ 10 é¡µï¼‰"
    )
    parser.add_argument(
        "--no-progress",
        action="store_true",
        help="ç¦ç”¨ tqdm è¿›åº¦æ¡ï¼ˆé»˜è®¤å¼€å¯ï¼‰"
    )
    parser.add_argument(
        "--github",
        default=None,
        help="GitHub ä»“åº“åœ°å€ï¼ˆå¦‚æœªæŒ‡å®šï¼Œè‡ªåŠ¨ä»è®ºæ–‡æ‘˜è¦æå–ï¼‰"
    )
    parser.add_argument(
        "--no-code",
        action="store_true",
        help="è·³è¿‡ä»£ç ä¸‹è½½ï¼Œä»…è½¬æ¢è®ºæ–‡"
    )
    parser.add_argument(
        "--code-only",
        action="store_true",
        help="ä»…æå–ä»£ç ï¼ˆè·³è¿‡è®ºæ–‡è½¬æ¢ï¼Œè¦æ±‚è®ºæ–‡å·²å­˜åœ¨ï¼‰"
    )
    args = parser.parse_args()

    tmp_pdf_path = None
    tmp_marker_dir = None
    global tqdm
    if args.no_progress:
        tqdm = None

    try:
        # 0. ç¡®å®šè¾“å‡ºç›®å½•
        if args.out_dir:
            base_dir = Path(args.out_dir).expanduser().resolve()
        else:
            base_dir = Path("Ideas").resolve()

        # 1. è§£æè¾“å…¥
        arxiv_id = _parse_arxiv_id(str(args.input).strip())
        if arxiv_id:
            fallback_name = arxiv_id
            pdf_stem = None
        else:
            pdf_path = Path(args.input).expanduser().resolve()
            if not pdf_path.exists():
                _log(f"âŒ PDF ä¸å­˜åœ¨ï¼š{pdf_path}")
                return 2
            if pdf_path.suffix.lower() != ".pdf":
                _log(f"âŒ ä¸æ˜¯ PDFï¼š{pdf_path}")
                return 2
            fallback_name = pdf_path.stem
            pdf_stem = pdf_path.stem

        # 2. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è½¬æ¢å¥½çš„è®ºæ–‡
        existing_md, existing_folder = _find_existing_project(base_dir, arxiv_id, pdf_stem if not arxiv_id else None)

        if existing_md and not args.overwrite:
            _log(f"ğŸ“„ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„è®ºæ–‡: {existing_md}")
            _log(f"ğŸ“ é¡¹ç›®ç›®å½•: {existing_folder}")

            # è¯»å–å·²æœ‰çš„ markdown å†…å®¹ç”¨äºä»£ç æå–
            md_content = existing_md.read_text(encoding="utf-8")
            folder_name = existing_folder
            final_file = existing_md

            # å¦‚æœæŒ‡å®šäº† --code-only æˆ–è®ºæ–‡å·²å­˜åœ¨ï¼Œç›´æ¥è·³åˆ°ä»£ç æå–
            _log("â­ï¸ è·³è¿‡è®ºæ–‡è½¬æ¢ï¼Œç›´æ¥è¿›å…¥ä»£ç æå–é˜¶æ®µ")

        else:
            # éœ€è¦è½¬æ¢è®ºæ–‡
            if args.code_only:
                _log(f"âŒ --code-only æ¨¡å¼ä½†æœªæ‰¾åˆ°å·²å­˜åœ¨çš„è®ºæ–‡")
                return 2

            # ä¸‹è½½ PDFï¼ˆå¦‚æœæ˜¯ arXivï¼‰
            if arxiv_id:
                tmp_pdf_path = _download_arxiv_pdf(arxiv_id)
                pdf_path = tmp_pdf_path

            # åˆ›å»ºä¸´æ—¶ç›®å½•è¿è¡Œ marker
            tmp_marker_dir = Path(tempfile.mkdtemp(prefix="p2m_marker_"))
            md_path = _run_marker(pdf_path, tmp_marker_dir, args.page_range)

            # è¯»å– markdown å†…å®¹
            md_content = md_path.read_text(encoding="utf-8")

            # ç”Ÿæˆè§„èŒƒçš„æ–‡ä»¶å¤¹åç§°
            folder_name = _generate_folder_name(md_content, fallback_name)
            _log(f"ğŸ“ æœ€ç»ˆæ–‡ä»¶å¤¹åç§°: {folder_name}")

            # è¾“å‡ºåˆ° Ideas/<PaperName>/Paper/ å­ç›®å½•
            final_dir = base_dir / folder_name / "Paper"

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if final_dir.exists() and not args.overwrite:
                _log(f"âŒ è¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼ˆå¯ç”¨ --overwrite è¦†ç›–ï¼‰ï¼š{final_dir}")
                return 1

            # ç§»åŠ¨æ–‡ä»¶åˆ°æœ€ç»ˆä½ç½®
            if final_dir.exists() and args.overwrite:
                shutil.rmtree(final_dir)
            final_dir.mkdir(parents=True, exist_ok=True)

            # å¤åˆ¶ md åŒç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬å›¾ç‰‡ï¼‰
            md_parent = md_path.parent
            items = list(md_parent.iterdir())
            pbar = (
                tqdm(total=len(items), desc="Collect output", unit="item", file=sys.stderr)
                if tqdm is not None and items
                else None
            )
            copied_files = []
            for item in items:
                if item.is_file():
                    if item.suffix.lower() == ".md":
                        # md æ–‡ä»¶é‡å‘½åï¼Œå¹¶ä¿®å¤å›¾ç‰‡è·¯å¾„
                        dest = final_dir / f"{folder_name}.md"
                        md_text = item.read_text(encoding='utf-8')
                        md_text = _fix_image_paths(md_text)
                        dest.write_text(md_text, encoding='utf-8')
                    else:
                        # å…¶ä»–æ–‡ä»¶ï¼ˆå›¾ç‰‡ç­‰ï¼‰ä¿æŒåŸå
                        dest = final_dir / item.name
                        shutil.copy2(item, dest)
                    copied_files.append(dest.name)
                elif item.is_dir():
                    # å¤åˆ¶å­ç›®å½•ï¼ˆå¦‚ _images ç›®å½•ï¼‰
                    dest_dir = final_dir / item.name
                    if tqdm is not None:
                        copied = _copy_tree_with_progress(item, dest_dir)
                        copied_files.append(f"{item.name}/ ({copied} files)")
                    else:
                        shutil.copytree(item, dest_dir)
                        copied_files.append(f"{item.name}/")
                if pbar is not None:
                    pbar.update(1)
            if pbar is not None:
                pbar.close()

            if len(copied_files) > 1:
                _log(f"ğŸ“¦ å¤åˆ¶äº† {len(copied_files)} ä¸ªæ–‡ä»¶/ç›®å½•")

            final_file = final_dir / f"{folder_name}.md"
            _log(f"âœ… è®ºæ–‡ä¿å­˜: {final_file}")

        # ============== Phase 2: ä»£ç ä¸‹è½½ä¸è§„èŒƒåŒ– ==============
        if not args.no_code:
            # è·å– GitHub URL
            github_url = args.github or _extract_github_url(md_content)

            if github_url:
                _log(f"ğŸ”— æ£€æµ‹åˆ° GitHub: {github_url}")

                # è·å–ä»“åº“ç»“æ„
                _log("ğŸ“‚ è·å–ä»“åº“ç»“æ„...")
                repo_tree = _get_repo_tree(github_url)

                if repo_tree:
                    # Codex è¯†åˆ«æ ¸å¿ƒæ–‡ä»¶
                    _log("ğŸ¤– Codex åˆ†ææ ¸å¿ƒæ–‡ä»¶...")
                    core_files = _identify_core_files(repo_tree, folder_name)

                    if core_files:
                        codes_dir = base_dir / folder_name / "Codes"
                        downloaded, file_mapping = _download_code_files(github_url, core_files, codes_dir)
                        if downloaded:
                            # Phase 2.5: è§£æå¹¶ä¸‹è½½ä¾èµ–
                            _log("ğŸ” è§£æä»£ç ä¾èµ–...")
                            downloaded, file_mapping = _resolve_dependencies(
                                codes_dir, github_url, repo_tree,
                                downloaded, file_mapping, max_depth=2
                            )

                            # Phase 3: è§„èŒƒåŒ–ä»£ç ï¼ˆCodex å¤„ç†ï¼‰
                            _log("ğŸ”§ è§„èŒƒåŒ–ä»£ç ...")
                            _codex_normalize_code(codes_dir, folder_name, md_content)

                            # æ›´æ–°å…ƒæ•°æ®ï¼ˆåŒ…å«ä¾èµ–ï¼‰
                            meta = {
                                "source": github_url,
                                "branch": "main",
                                "files": file_mapping,
                                "downloaded_at": datetime.now().isoformat()
                            }
                            meta_file = codes_dir / "_source.json"
                            meta_file.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding='utf-8')
                            _log(f"âœ… ä»£ç ä¿å­˜: {codes_dir} ({len(downloaded)} æ–‡ä»¶)")
                        else:
                            _log("âš ï¸ æœªèƒ½ä¸‹è½½ä»»ä½•ä»£ç æ–‡ä»¶")
                    else:
                        _log("âš ï¸ æœªè¯†åˆ«åˆ°æ ¸å¿ƒæ¶æ„æ–‡ä»¶")
                else:
                    _log("âš ï¸ æ— æ³•è·å–ä»“åº“ç»“æ„ï¼ˆå¯èƒ½æ˜¯ç§æœ‰ä»“åº“æˆ–ç½‘ç»œé—®é¢˜ï¼‰")
            else:
                _log("â„¹ï¸ æœªæ£€æµ‹åˆ° GitHub URLï¼ˆå¯ç”¨ --github æŒ‡å®šï¼‰")

        print(f"âœ… è¾“å‡ºæ–‡ä»¶ï¼š{final_file}")
        return 0

    except Exception as e:
        _log(f"âŒ é”™è¯¯ï¼š{e}")
        return 1

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        _cleanup_file(tmp_pdf_path)
        if tmp_marker_dir and tmp_marker_dir.exists():
            try:
                shutil.rmtree(tmp_marker_dir)
            except Exception:
                pass


if __name__ == "__main__":
    raise SystemExit(main())
